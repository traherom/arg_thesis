\chapter{Results and Analysis}
\label{chp:results}
\par This chapter presents and analyzes the experimental results. Section \ref{sec:results_basic_tests} covers the basic functionality tests, Section \ref{sec:results_hoprate_tests} determines the effects of hop interval and latency, and Section \ref{sec:results_packetrate_tests} discusses \ac{ARG}'s performance. Section \ref{sec:results_fuzzer_tests} covers the results of running a fuzzer against \ac{ARG}. Finally, Section \ref{sec:results_overall} revisits the research questions Chapter \ref{chp:methodology} poses and summarizes the results with respect to each of them.

<<echo=FALSE, message=FALSE>>=
library(gplots)
library(plotrix)
library(boot)
library(coin)
library(cluster)
options(scipen=10)
@

<<echo=FALSE>>=
alpha=.95
acceptable=2
replicates=1000

# Bring in CSV file and break out into each test, excluding the results.dir and
# label columns
results = read.csv('results.csv', header=TRUE)
results = results[,1:ncol(results)-1] # Remove blank column at end

# Proportions are small and silly
loss_cols = c(colnames(results)[startsWith(colnames(results), 'valid.loss.rate')], 'invalid.loss.rate')
results[,loss_cols] = results[,loss_cols] * 100

# Traffic flows were 0-indexed. Make them 1-indexed for readability
#results[,'test'] = results[,'test'] + 1

# Convert ms values to integers
results[,'hop.rate'] = as.integer(sapply(results[,'gatea.hop.rate'], function(x) {
		x = as.character(x);
		substr(x, 1, nchar(x)-2);
	}))

# Combine stats to make them easy to access
results[,'kbps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.kbps'], x['gateb.kbps'], x['gatec.kbps'])
	)) })
results[,'pps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.pps'], x['gateb.pps'], x['gatec.pps'])
	)) })

results = split(results, results['label'])
basic = results$basic
hoprate = results$hoprate
packetrate = results$packetrate
fuzzer = results$fuzzer
@

\section{Basic Tests}
\label{sec:results_basic_tests}
\par The first set of experiments test the basic functionality of \ac{ARG}, as Section \ref{sec:exp_design} discusses. These tests are intended to verify the basic functionality of \ac{ARG} and determine if it classifies traffic correctly, per the expected traffic flow. Given that stressing the system is not a goal of this series of tests, packet delay is set to a high value of 0.3 seconds, resulting in a slow packet rate. (Packet rate stressing is left until the throughput tests in Section \ref{sec:results_packetrate_tests}.) Hop intervals vary between 50 milliseconds and 500 milliseconds, with a \ac{RTT} fixed at 20 milliseconds.

<<echo=FALSE>>=
bootstrap_stats = function (data, idx) {
	m = mean(data[idx], na.rm=TRUE)
	return(m)
}

# Calculate overall average
basic.boot.overall = boot(data=basic[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
basic.boot.overall.stat = basic.boot.overall$t0
basic.boot.overall.ci = boot.ci(basic.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
basic.boot = by(basic[,'valid.loss.rate'], basic[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
basic.boot.stats = sapply(basic.boot, function(x) x$t0)
basic.boot.ci = sapply(basic.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
boot.t.test = function(x, y, R, n=10, conf=.95) {
	# Create replicates
	x_resampled = list()
	y_resampled = list()
	for(i in 1:R) {
		x_resampled[[i]] = sample(x, n)
		y_resampled[[i]] = sample(y, n)
	}

	# Test
	t = c()
	for(i in 1:R) {
		t = c(t, t.test(x_resampled[[i]], y_resampled[[i]], conf.level=conf)$statistic)
	}

	# Confidence intervals
	t = sort(t)
	cutoff = floor((1-conf) * length(t))
	return(c(t[cutoff], t[length(t) - cutoff]))
}

basic.hr = split(basic, basic[,'hop.rate'])
basic.hr.t = boot.t.test(basic.hr$'50'[,'valid.loss.rate'], basic.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
basic.hr.pvalue = pvalue(oneway_test(basic[,'valid.loss.rate']~factor(basic[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

# Utility to show why packets were rejected
create_rejection_table = function(name, caption, table) {
	# Total number of packets
	total_packets = sum(table[,'valid.sent'], table[,'invalid.sent'])

	# Sum just the rejection reason columns
	reasons = table[, sapply(colnames(table), function(x) {
			return(startsWith(x, 'inbound') || startsWith(x, 'outbound')
					|| x == 'unknown' || x == 'lost.on.wire')
		})]
	reasons = apply(reasons, 2, function(x) sum(x, na.rm=TRUE))
	reasons = sort(reasons, decreasing=TRUE)

	# Table header with caption
	cat('\\begin{table}\n')
	cat('\\caption[')
	cat(caption)
	cat(']{')
	cat(caption)
	cat('. ') 
	cat(nrow(table))
	cat(' tests with ')
	cat(prettyNum(total_packets, big.mark=','))
	cat(' total packets represented.')
	cat('}\n')
	cat('\\label{tbl:')
	cat(name)
	cat('}\n')
	cat('\\centering\n')
	cat('\\begin{tabular}{l|c|c}\n')
	cat('\\textbf{Reason} & \\textbf{Count}  & \\textbf{\\% of Total Packets}\\\\\n')
	cat('\\hline\n')

	# Reasons
	for(i in 1:length(reasons)) {
		if(is.na(reasons[i]) || reasons[i] == 0)
			next
		
		r = names(reasons)[i]
		r = gsub('.', ' ', r, fixed=TRUE)
		r = gsub(' nat ', ' \\ac{NAT} ', r, fixed=TRUE)
		r = gsub(' ip ', ' \\ac{IP} ', r, fixed=TRUE)
		r = gsub(' dest ', ' destination ', r, fixed=TRUE)
		r = paste(toupper(substr(r, 1, 1)), substring(r, 2), sep='')

		cat(r, ' & ', prettyNum(reasons[i], big.mark=','), ' & ', reasons[i]/total_packets*100, '\\%', sep='')
		if(i < length(reasons))
			cat('\\\\\n')
	}

	cat('\\end{tabular}\n')
	cat('\\end{table}\n')

	return(c(nrow(table), total_packets))
}
@

\subsection{Valid Packet Loss}
\label{sec:basic_valid_packet_loss}
\par The raw results for the loss of valid packets on each test are shown in Figure \ref{fig:basic_raw_data}. Figure \ref{fig:basic_mean_ci} shows the mean and \acp{CI} of each test, using a \Sexpr{alpha*100}\% confidence level. Due to a lack of normality in the experimental results, these numbers are calculated via bootstrapping with \Sexpr{replicates} replicates.

\begin{figure}
\caption{Basic tests, raw valid packet loss}
\label{fig:basic_raw_data}
\centering
<<basic_tests_valid_raw, echo=FALSE>>=
# Graph and generate stats on basic tests with test num vs loss  
ylim=c(0, max(basic[,'valid.loss.rate'], na.rm=TRUE))

plot(basic[,'test'], basic[,'valid.loss.rate'],
	xlab="Traffic Flow Number", xaxt='n',
	ylab="Valid Packet Loss", ylim=ylim, yaxt='n')
axis(1, at=names(basic.boot), labels=names(basic.boot))
ylab = seq(0, max(ylim), .5)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}

\begin{figure}
\caption{Basic tests, valid packet loss means and confidence intervals}
\label{fig:basic_mean_ci}
\centering
<<basic_tests_valid_mean, echo=FALSE, warning=FALSE>>=
ylim=c(0, max(basic.boot.ci, na.rm=TRUE))

plot.new()
plot.window(xlim=c(0, length(basic.boot)-1), ylim=ylim)
title(main="", xlab="Traffic Flow Number", ylab="Mean Valid Packet Loss")

axis(1, at=names(basic.boot), labels=names(basic.boot))
ylab = seq(0, max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
box()

points(0:(length(basic.boot)-1), basic.boot.stats)
plotCI(0:(length(basic.boot)-1), basic.boot.stats,
	ui=sapply(basic.boot.ci[2,], function(x) min(x, 1)),
	li=sapply(basic.boot.ci[1,], function(x) max(x, 0)),
	add=TRUE, scol='blue')

#text(0:(length(basic.boot)-1) + .5, basic.boot.stats,
#	labels=sapply(basic.boot.stats, function(x) round(x, digits=4)))
@
\end{figure}

\par These figures reveal a low packet loss across all test scenarios. At worst, Flow 3---TCP traffic between \ac{ARG} protected clients only---lost between \Sexpr{basic.boot.ci[1,4]}\% and \Sexpr{basic.boot.ci[2,4]}\% of packets, with \Sexpr{alpha*100}\% confidence. Across all tests, valid packet loss averaged \Sexpr{basic.boot.overall.stat}\%, with a \ac{CI} of $(\Sexpr{I(basic.boot.overall.ci[1])}\%, \Sexpr{I(basic.boot.overall.ci[2])}\%)$, again with \Sexpr{alpha*100}\% confidence. These results confirm Chapter \ref{chp:methodology}'s hypothesis that \ac{ARG} causes packet loss less than \Sexpr{acceptable}\%, at least under normal conditions.

\par Figure \ref{fig:basic_raw_data} displays two outliers on Flow 3. Examining the data reveals the dropped packets exceeded the maximum size of a packet \ac{ARG} can pass between gateways. As Chapter \ref{chp:implementation} covers, \ac{ARG} wraps packets between gateways in its own headers, increasing the size of the packet. Ethernet II has a default maximum transmission unit of 1500 bytes, so \ac{ARG} drops packets that pass this limit after being wrapped (\ac{ARG} is unable to fragment packets or inform the sender that fragmentation is needed, as Section \ref{sec:future_work} mentions). In fact, the maximum packet size issue is the most common reason for packet rejection in the basic set of tests (in tests with only valid traffic), as shown in Table \ref{tbl:basic_loss_reasons_valid}. While test traffic is controlled for size, an oversight in the test traffic generators fails to account for \ac{TCP} options, which can increase the size of the packet beyond the limit. \ac{TCP} options here refers to the optional header data after the main \ac{TCP} headers \cite{rfc793}.

<<echo=FALSE, results='asis'>>=
valid_runs = apply(basic, 1, function(x) x['test'] < 5)
num_runs = nrow(basic[valid_runs,])
tots = create_rejection_table('basic_loss_reasons_valid', 'Basic tests 1-4 packet rejection reasons', basic[valid_runs,])
@

\par Other entries in this table deserve some discussion. The next most common reason, incorrect sequence numbers, indicates that \ac{ARG}'s replay protection mechanism blocked packets. This commonly occurs at initialization, resulting in an average of \Sexpr{round(596/tots[1])} sequence number issues per test over the course of \Sexpr{tots[1]} tests, one for each gateway. The third entry on the list, ``Inbound Unwrapped,'' occurs at the end of a test, where the receiver of a packet is shutdown before the sender. The results processor traces packets through the network using a combination of packet captures and text log files of actions on each host. When a test run ends, these utilities shutdown sequentially. If a host sends a packet to a gateway that is still recording packet captures but has already shutdown its application-level receiver script (which creates the text log files), the test processor returns the last message it saw regarding the packet, which is typically whatever action the gateway took. Appendix \ref{chp:processor} covers the test processor in more detail. The end-of-test issues are also the case for the ``outbound rewrite,'' ``ping accepted,'' and ``unknown'' entries. The remainder of the reasons are self explanatory.

\par A resampling of the data between the two hop intervals used in the basic tests gives a p-value of \Sexpr{round(basic.hr.pvalue, 3)}, indicating little significant difference between the two hop intervals. This confirms that hop interval in and of itself does not affect losses on the network, an important fact when considering the minimum hop interval in Section \ref{sec:results_hoprate_tests}. %convincing evidence that there is a statistically significant difference introduced by the different hop rates. However, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms, just barely twice the 20 ms latency). Ideally these tests would be re-run with a hop rate closer to 100 milliseconds to avoid interaction with the latency.

%However, the \ac{CI} for this difference is a mere $(\Sexpr{I(ci_hr)})$ at a \Sexpr{alpha*100}\% confidence level, fairly small for the purposes of network traffic. Additionally, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms with a latency of 20 ms).

\FloatBarrier
\subsection{Invalid Packet Loss}
\par Traffic Flows 5 through 8 include invalid traffic that \ac{ARG} should reject and hence should be ``lost.'' Table \ref{tbl:basic_invalid_loss} displays the percentage of invalid traffic rejected (with 95\% \acp{CI}), clearly revealing \ac{ARG} has an extremely low false negative rate. As shown, over the course of 28 repetitions only one test appears to have allowed a single packet through. %\Sexpr{alpha*100}\% \ac{CI} are theoretically displayed, but all showed 100\% invalid packet loss.

<<echo=FALSE, results='hide'>>=
# Calculate overall average
basic.inv.boot.overall = boot(data=basic[,'invalid.loss.rate'], statistic=bootstrap_stats, R=replicates)
basic.inv.boot.overall.stat = basic.inv.boot.overall$t0
basic.inv.boot.overall.ci = boot.ci(basic.inv.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
basic.inv.boot = by(basic[,'invalid.loss.rate'], basic[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
basic.inv.boot.stats = sapply(basic.inv.boot, function(x) x$t0)
basic.inv.boot.ci = sapply(basic.inv.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})
@

\begin{comment}
\begin{figure}
\caption{Invalid traffic packet loss}
\label{fig:basic_invalid_loss}
\centering
<<basic_tests_invalid_mean, out.width=".75\\textwidth", echo=FALSE, warning=FALSE>>=
# Show loss of invalid traffic for tests 5-8.
invalid_runs = !valid_runs
ylim = c(99.9, 100)
plotmeans(basic[invalid_runs, 'invalid.loss.rate']~basic[invalid_runs, 'test'], p=alpha,
		connect=FALSE,
		minbar=0, maxbar=100, ylim=ylim, yaxt='n',
		xlab="Traffic Flow Number", ylab="Invalid Packet Loss")
ylab = seq(0, max(ylim), .01)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}
\end{comment}

\begin{table}
\caption{Basic tests, packet loss of invalid traffic}
\label{tbl:basic_invalid_loss}
\centering
\begin{tabular}{l|c|c|c}
\textbf{Flow} & \textbf{Mean} & \textbf{\ac{CI}} & \textbf{Replications}\\
\hline
<<results='asis', echo=FALSE>>=
invalid_runs = !valid_runs
invalid_tests = c(5:8)
for(test in invalid_tests) {
	cat('Flow ')
	cat(test)
	cat(' & ')

	# Mean
	cat(basic.inv.boot.stats[test+1])
	cat('\\% & ')

	# CI
	ci = unlist(basic.inv.boot.ci[test+1])
	if(!is.null(ci)) {
		cat('(')
		cat(ci[1])
		cat('\\%, ')
		cat(ci[1])
		cat('\\%)')
	} else {
		cat('(100\\%, 100\\%)')
	}

	# Replications
	cat(' & ')
	cat(sum(basic[,'test'] == test))

	if(test < max(invalid_tests))
		cat('\\\\\n')
}
@
\end{tabular}
\end{table}

\par Flow 8 shows a small deviation from the expected 100\% packet rejection, as 1 of 746 packets made it through, but further examination of this number shows a rare post-processing problem. If two identical packets are sent at the same time, the test run processor leaves the second one unmarked, which is interpreted later as successfully received. The problem lies with the log analyzer, not \ac{ARG} itself, which did in fact reject every packet. This result offers convincing evidence that \ac{ARG} effectively blocks unexpected inbound traffic, but it gives no indication of its suitability against more focused attacks. 

<<echo=FALSE, results='asis'>>=
# Get the total for each possible rejection reason for the invalid tests only
tots = create_rejection_table('basic_loss_reasons_invalid', 'Basic tests 5-8 packet rejection reasons', basic[invalid_runs,])
@

\par Additionally, Table \ref{tbl:basic_loss_reasons_invalid} illustrates that packets are rejected via the \ac{NAT} table quite frequently over the \Sexpr{tots[1]} test runs with invalid traffic. This operation costs the gateway minimal processing time, as it can reject after a single hash table lookup. Fast decisions on packets lower the possibility of an effective \ac{DOS} attack, as the \ac{CPU} can continue to transform valid traffic. Section \ref{sec:basic_valid_packet_loss} discusses the meaning of the remainder of this table.

\FloatBarrier
\section{Minimum Hop Interval}
\label{sec:results_hoprate_tests}
\par The minimum hop interval sequence of tests measures the change in packet loss at specific latencies as the time between hops decreases. For these tests, a fixed packet rate and Flow 4 are always used. Section \ref{sec:exp_design} covers the specifics of these tests. The figures below document the results of the tests, broken up by traffic type and direction.

<<echo=FALSE>>=
# Change latency to one-way (currently RTT)
hoprate[,'latency'] = hoprate[,'latency'] / 2

plot_latency_hoprate = function(hoprate, loss_rate_col, title='', xlim=NULL, ylim=NULL, yby=2) {
	# Get mean of each hop interval-latency pair. Matrix
	# rows are latencies, columns are hop intervals 
	lat_hr_mean = matrix(nrow=length(unique(hoprate[,'latency'])),
						ncol=length(unique(hoprate[,'hop.rate'])))
	rownames(lat_hr_mean) = sort(unique(hoprate[,'latency']))
	colnames(lat_hr_mean) = sort(unique(hoprate[,'hop.rate']))

	worst_sd = 0
	repetitions = c()
	for(lat in rownames(lat_hr_mean)) {
		for(hr in colnames(lat_hr_mean)) {
			rows = hoprate[,'latency'] == lat & hoprate[,'hop.rate'] == hr

			# Calculate mean for each test number
			#b = boot(data=hoprate[rows,], statistic=function(d, i) {
			#			return(c(mean(d[i], na.rm=TRUE), sd(d[i], na.rm=TRUE)))
			#		}, R=replicates)

			lat_hr_mean[lat, hr] = mean(hoprate[rows, loss_rate_col])
			#lat_hr_mean[lat, hr] = b$t0

			repetitions = append(repetitions, length(hoprate[rows, loss_rate_col]))

			curr_sd = sd(hoprate[rows, loss_rate_col])
			#curr_sd = b$t1
			if(!is.na(curr_sd) && curr_sd > worst_sd)
				worst_sd = curr_sd
		}
	}

	# Plot. Each latency is separate, x is hop interval, y is loss
	old_par = par(no.readonly=TRUE)
	par(mar=c(5.1, 4.1, 4.1, 7.8))

	plot.new()
	if(is.null(xlim))
		xlim = c(1, ncol(lat_hr_mean))
	if(is.null(ylim))
		ylim = c(0, max(lat_hr_mean, na.rm=TRUE))
	plot.window(xlim=xlim, ylim=ylim)
	title(main=title, xlab="Hop Interval (ms)", ylab="Packet Loss")

	axis(1, at=1:ncol(lat_hr_mean), labels=colnames(lat_hr_mean), cex.axis=0.75)
	yaxis = c(acceptable, seq(from=min(ylim), to=max(ylim), by=yby))
	axis(2, at=yaxis, labels=paste(yaxis, '%', sep=''))

	box()

	abline(h=acceptable, lty=5, col="purple")

	type=1
	for(lat in rownames(lat_hr_mean)) {
		lines(lat_hr_mean[lat,], col=type, lty=type)
		points(lat_hr_mean[lat,], col=type, pch=type-1)
		type = type + 1
	}

	legend(title="One-way Latency",
			xpd=TRUE,
			x=xlim[2] + .6,
			y=mean(ylim) + (ylim[2] - ylim[1])/5,
			legend=c(paste(rownames(lat_hr_mean), "ms"), paste('Acceptable\nLoss: ', acceptable, '%', sep='')),
			lty=c(1:(type-1), 5),
			col=c(1:(type-1), "purple"),
			pch=c(0:(type-2), NA)
			)

	par(old_par)

	return(c(sd=worst_sd, repetitions=mean(repetitions)))
}
@

\begin{figure}
\caption[Hop interval tests, packet loss of \ac{UDP} and \ac{TCP} traffic between \ac{ARG} networks]{Hop interval tests, packet loss of \ac{UDP} and \ac{TCP} traffic between \ac{ARG} networks. Note: X axis is not linear.}
\label{fig:hrlat_udptcp_inter_full}
\centering
<<hop_rate_udptcp_interarg, echo=FALSE>>=
plot_results = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', yby=5)
worst_sd = plot_results['sd']
repetitions = plot_results['repetitions']
@
\end{figure}

\begin{figure}
\caption{Hop interval tests, scaled view of packet loss between \ac{ARG} networks}
\label{fig:hrlat_udptcp_inter_zoomed}
\centering
<<hop_rate_udptcp_interarg_scaled, echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', ylim=c(0, 5), yby=.5)
@
\end{figure}

\par Figure \ref{fig:hrlat_udptcp_inter_full} displays the results of the hop interval tests, separated by network latency. Figure \ref{fig:hrlat_udptcp_inter_zoomed} displays the same numbers, but with a much tighter Y scale to reveal differences at large hop intervals. The numbers shown here only include packets sent between \ac{ARG}-protected clients; traffic to the external host and administrative packets between gateways are not included (packets to external hosts are considered separately later). The highest standard deviation in any of the hop interval-latency groups is \Sexpr{worst_sd}, implying the potential for significant (but still potentially acceptable) variation in network performance. %implying some variation in each hop rate-latency grouping but still relatively predictable losses.

\par Interestingly, the zero millisecond latency trials exhibit a noticeably different decline than the others, with zero milliseconds giving a relatively steady decline in packet loss and the others remaining relatively flat before a rapid drop. The 15 millisecond latency curve gives a hint the shapes may not be as different as they appear, as it shows two or three mid-way data points on the way down to acceptable packet loss. With tests against more hop intervals, these curves would likely show a more gradual (although still rapid) decline. 

\par Chapter \ref{chp:methodology} hypothesizes that packet loss reaches acceptable levels when the time between hops is equal to (or exceeds) the one-way network latency. Theoretically, this time frame should allow a packet to cross the network from one gateway to another before the receiver changes \ac{IP} addresses twice, even if the packet is sent just before the receiver hops the first time (as mentioned in Chapter \ref{chp:implementation}, gateways accept packets at the current and previous \ac{IP} \tbd{mullins wants a diagram of this? Illustrating packet sending and receiving, time lapse, etc}). Any shorter hop interval results in a period of time before each hop that guarantees sent packets arrive too late.  Figure \ref{fig:hrlat_udptcp_inter_zoomed}, however, shows that loss drops below \Sexpr{acceptable}\% when the hop interval is around four times the latency. This difference from the anticipated behavior is likely the result of the test environment.

\par As Section \ref{sec:eth_routing} discusses, Ethernet uses \ac{ARP} requests and responses to determine the source and destination for packets on the local network. In the test environment, the gateways and the external host are all on the same local network. This results in extra Ethernet \ac{ARP} requests and responses, when compared to operation over the Internet. On a normal Ethernet segment with minimal latency (the zero millisecond test), the \ac{ARP} process takes minimal time and has little impact on operation. To speed the process even further, \ac{ARP} data is cached, so when a system wants to send to the same \ac{IP} again, it references the ARP cache table and does not have to go through the request-response process again. With short hop intervals, however, gateways frequently need to send to different \ac{IP} addresses and therefore must send and receive the requisite \acp{ARP} very frequently (generally, once every hop). When the network latency is not zero, these additional frames can cause significant delays. Figure \ref{fig:triple_arp_issue} illustrates what happens when a time synchronization between gateways occurs with a five millisecond one-way latency.

\begin{figure}
\caption{Time synchronization process, including \acp{ARP}}
\label{fig:triple_arp_issue}
\noindent\makebox[\textwidth]{%
\centering
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue}
\caption{With 5 millisecond one-way universal latency, including \acp{ARP}, as occurs on the test network.}
\end{subfigure}
\hspace{.5pt}
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue_no_arp}
\caption{With 5 millisecond latency across the network and insignificant \acp{ARP} (not shown), as might occur in real operation across the Internet.}
\end{subfigure}
}
\end{figure}

\par If there is no need for these ARP requests (or they were negligible, time-wise), the entire time synchronization process completes in 10 milliseconds. Because \acp{ARP} on the test network experience the same delay as \ac{UDP} and \ac{TCP} packets, however, it takes 30 milliseconds to synchronize times. The time base for the other gateway is calculated correctly despite the extra delay because the latency of that particular packet is taken into account. Later packets may take a wide variety of latencies, however, because they may not require any \acp{ARP}, only one direction might require it, or both directions may require it. For a 5 millisecond one-way latency, this leads to variations of 5, 10, or 15 milliseconds, an acceptable and tolerable range. By the time tests get to set latencies of 250 milliseconds, one-way latencies range from 250 to 750 milliseconds, with \acp{RTT} of up to 1500 milliseconds. This degree of variation is difficult to compensate for, leading to significant packet loss until the hop interval exceeds three times the set latency. Additionally, at shorter hop intervals the triple-latency problem is encountered more frequently, as the ARP cache must be refreshed more often. 

\par On the Internet, the triple-latency problem does not occur. An \ac{ARG} gateway sending a packet to another gateway uses \ac{ARP} requests to get the hardware address of the local router, a process which takes under a millisecond on most Ethernet networks. Once the \ac{ARG} gateway has the hardware address of the router, it sends the data packet out to the other gateway. After the packet leaves the local network, \ac{IP} address hopping does not affect network operation at all, resulting in the packets between gateways experiencing same latency as any other data on the network. A 30 millisecond one-way latency across the Internet (or other \ac{WAN}) means packets genuinely take (on average) 30 milliseconds to reach their destination, whereas a 30 millisecond latency on the test network often means packets take 90 milliseconds.

\par Despite having no latency, the zero millisecond test exhibits high loss until around 75 milliseconds hops, similar to when the 15 millisecond latency test indicates a usable connection. This may indicate a that hopping more frequently than every 50-75 milliseconds is impractical regardless of network conditions with the current implementation. It may be possible for the sender of a packet to calculate ``future'' \ac{IP} addresses to use for each packet sent, so that the addresses are current when they arrive at the destination. This is an area for future research, as Section \ref{sec:future_work} discusses. Previous research into address space randomization and \ac{IP} hopping limit themselves to hopping on the order of minutes or hours and never explore the maximum rate possible.

\par \ac{ARG} appears to have little additional impact on \ac{TCP}, when compared to \ac{UDP}. Figure \ref{fig:hrlat_tcp_inter} displays losses associated with only \ac{TCP} packets travelling between gateways. Compared to Figure \ref{fig:hrlat_udptcp_inter_full} (which includes UDP and TCP packets), trends appear similar. 

\begin{figure}
\caption{Hop interval tests, packet loss of \ac{TCP} traffic between \ac{ARG} networks} 
\label{fig:hrlat_tcp_inter}
\centering
<<hop_rate_tcp_interarg, echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.tcp.interarg', yby=5)
@
\end{figure}

\par Table \ref{tbl:hoprate_loss_reasons} shows the packet rejection reasons for all hop interval tests. As expected, far and away the most common rejection reason for this set of tests is incorrect source and/or destination \acp{IP}. Other entries in this table are discussed in Section \ref{sec:basic_valid_packet_loss}.

<<echo=FALSE, results='asis'>>=
tots = create_rejection_table('hoprate_loss_reasons', 'Hop interval test loss reasons', hoprate)
@

\par Finally, Figure \ref{fig:hrlat_extra} demonstrates there is little correlation between the hop internal and the flow of traffic to external hosts. Over all \Sexpr{nrow(hoprate)} hop interval test runs, packet loss to and from the external host never exceeds \Sexpr{signif(max(hoprate[,'valid.loss.rate.extraarg']), 2)}\%. This makes sense, as the changing \ac{IP} is only referenced when the \ac{NAT} module first creates a table entry. Later packets on the same connection consult the table information and are unaware of any \ac{IP} address changes. 

\begin{figure}
\caption{Hop interval tests, packet loss of externally-bound traffic}
\label{fig:hrlat_extra}
\centering
<<hop_rate_extraarg, echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.extraarg', yby=.005)
@
\end{figure}

\section{Maximum Packet Rate}
\label{sec:results_packetrate_tests}
\par The final numerical test performed against \ac{ARG} measures the maximum packet rate it is capable of supporting. For these tests, latency is set to 20 milliseconds and Flow 4 is used. Hop intervals vary between 50 ms and 500 ms to check if this has an effect on the supported packet rate. Most importantly, traffic generators are run with steadily increasing send rates. Section \ref{sec:exp_design} contains more details. Because throughput varies somewhat on each run, the data is clustered into the groupings listed in Table \ref{tbl:pr_clusters} and colored in Figure \ref{fig:pr_raw_data}.

<<echo=FALSE>>=
# Sort base on throughput
packetrate = packetrate[order(packetrate[,'kbps']),]

# Cluster data by kbps
cols = c('kbps', 'valid.loss.rate')
nclusters = 7
packetrate.fit = kmeans(packetrate[,cols], nclusters, nstart=500)
packetrate[,'bps.clust'] = packetrate.fit$cluster

# Reorder cluster numbers to be increasing according to kbps
old_clusters = packetrate[,'bps.clust']
curr_cluster_start = 1
for(curr in 1:max(old_clusters)) {
	clust = old_clusters == old_clusters[curr_cluster_start]
	packetrate[clust,'bps.clust'] = curr
	curr_cluster_start = curr_cluster_start + sum(clust)
}

# Calculate mean kbps for each throughput cluster
packetrate.boot = by(packetrate[,'valid.loss.rate'], packetrate[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
packetrate.boot.stats = sapply(packetrate.boot, function(x) x$t0)
packetrate.boot.ci = sapply(packetrate.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
packetrate.hr = split(packetrate, packetrate[,'hop.rate'])
packetrate.hr.t = boot.t.test(packetrate.hr$'50'[,'valid.loss.rate'], packetrate.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
packetrate.hr.pvalue = pvalue(oneway_test(packetrate[,'valid.loss.rate']~factor(packetrate[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

clust_colors = c("black", "red", "green", "purple",
				 "blue", "magenta", "cyan")
@

\begin{table}
\caption{Packet rate clusters}
\label{tbl:pr_clusters}
\centering
\begin{tabular}{c|c|c|c}
\textbf{Clust} & \textbf{Mean (Kbps)}  & \textbf{Min (Kbps)}  & \textbf{Max (Kbps)} \\
\hline
<<echo=FALSE, results='asis'>>=
for(clust in unique(packetrate[,'bps.clust'])) {
	kbps = packetrate[packetrate[,'bps.clust'] == clust,'kbps']
	cat(clust)
	cat(' & ')
	cat(mean(kbps))
	cat(' & ')
	cat(min(kbps))
	cat(' & ')
	cat(max(kbps))
	cat('\\\\\n')
}
@
\end{tabular}
\end{table}

\begin{figure}
\caption{Packet rate tests, clustered loss verses throughput}
\label{fig:pr_raw_data}
\centering
<<max_rate_clustered, echo=FALSE>>=
# Plot 
ylim = c(0, .4)

plot(packetrate[,'kbps'], packetrate[,'valid.loss.rate'],
	ylim=ylim, yaxt='n',
	col=clust_colors[packetrate[,'bps.clust']],
	pch=packetrate[,'bps.clust'] - 1,
	xlab="Throughput (Kbps)", ylab="Valid Packet Loss",
	sub="Colors indicate clustering")
for(clust in unique(packetrate[,'bps.clust'])) {
	rows = packetrate[,'bps.clust'] == clust
	lines(packetrate[rows,'kbps'], packetrate[rows,'valid.loss.rate'], col=clust_colors[clust])

	# Divider line between each with number of cluster
	kbps_curr = packetrate[packetrate[,'bps.clust'] == clust,'kbps']
	kbps_next = packetrate[packetrate[,'bps.clust'] == clust + 1,'kbps']
	if(clust < nclusters) {
		abline(v=mean(c(max(kbps_curr), min(kbps_next))), lty=3)
	}

	# Label each cluster
	text(mean(kbps_curr), 0, clust, col=clust_colors[clust]) 
}

ylab = seq(min(ylim), max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))

max_packet_rate = max(packetrate[,'kbps'])
@
\end{figure}

\par Figure \ref{fig:pr_raw_data} shows the raw data from these tests, with colors indicating the \Sexpr{max(packetrate[,'bps.clust'])} K-means clusters, grouped by bit-wise throughput. Clusters are chosen via the Hartigan and Wong algorithm, with 500 random starts. This graph includes all packets on the network. Visually, the plot reveals fairly similar loss in each cluster, although some clusters exhibit a fair amount of variation. A Tukey test with each cluster confirms this result, as shown in Figure \ref{fig:pr_tukey}. 

\begin{figure}
\caption{Packet rate tests, Tukey test against clustered data}
\label{fig:pr_tukey}
<<max_rate_tukey, echo=FALSE>>=
m = aov(valid.loss.rate~factor(bps.clust), packetrate)
old_par = par(no.readonly=TRUE)
par(cex.axis=.5)
t = TukeyHSD(m, conf.level=alpha)
plot(t)
par(old_par)
@
\end{figure}

\par On Figure \ref{fig:pr_tukey}, numbers along the side correspond to the cluster numbers shown in Table \ref{tbl:pr_clusters} and Figure \ref{fig:pr_raw_data}. Six means stand out as being distinctly different: 1-4, 2-4, 3-4, 4-5, 4-6, and 4-7. Cluster 4 is statistically different from all other clusters, with \Sexpr{alpha*100}\% confidence. However, the other data gives reason to believe that throughput was not the cause of loss increase and variation. No changing trend is seen in any of the surrounding throughputs' losses (either up or down) and faster throughputs (Clusters 5 through 7) show loss comparable to the slower throughputs (Clusters 1 through 3). While the precise cause of the increased loss in some of these tests is unknown, the amount of data the gateways handle is not believed to be the cause.

\par The results of these tests are limited to just the implementation of \ac{ARG} and imply only a minimally possible throughput. Despite these limitations, \ac{ARG} performs well with reasonably rapid traffic of over four \ac{Mbps}, giving no reason to believe it is not capable of higher rates. Time constraints prohibited more rapid throughput tests.

\section{Fuzzing Test}
\label{sec:results_fuzzer_tests}
\par The final series of tests run on \ac{ARG} consist of sending invalid and replayed traffic to the gateways. The primary objective is to verify that \ac{ARG} remains stable and able to pass valid traffic while under attack. 

<<echo=FALSE>>=
# Calculate overall average
fuzzer.boot.overall = boot(data=fuzzer[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
fuzzer.boot.overall.stat = fuzzer.boot.overall$t0
fuzzer.boot.overall.ci = boot.ci(fuzzer.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
fuzzer.boot = by(fuzzer[,'valid.loss.rate'], fuzzer[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
fuzzer.boot.stats = sapply(fuzzer.boot, function(x) x$t0)
fuzzer.boot.ci = sapply(fuzzer.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})
@

\par The custom run-processing tool this thesis uses to generate packet loss statistics is unable to process the logs from the malicious traffic generators, making it difficult to determine precisely what occurs in each run. This calls into question the accuracy of the results from the run processor. Nonetheless, the tool reports a mean packet loss between \Sexpr{signif(fuzzer.boot.overall.ci[1], 3)}\% and \Sexpr{signif(fuzzer.boot.overall.ci[2], 3)}\% (with \Sexpr{alpha*100}\% confidence), which indicates \ac{ARG} experiences significant difficulty when faced with malicious traffic. A manual examination of log files does seem to indicate this is the case, with some traffic successfully traversing the network but a huge amount being rejected. 

\begin{comment}
Table \ref{tbl:fuzzer_loss_reasons} gives the reasons behind each rejection.

<<results='asis', echo=FALSE>>=
tots = create_rejection_table('fuzzer_loss_reasons', 'Fuzzer packet reject reasons', fuzzer)
@
\end{comment}

\par Despite the high losses, \ac{ARG} does appear to remain stable throughout the barrage. In all cases, \ac{ARG} never crashes and maintains around the same ability to handle valid traffic (that is, although the losses appear high, they remain consistent throughout). Further research is needed to identify the causes behind the high losses, but the problem likely lies in implementation and not architecture. Even if \ac{ARG} crashes, traffic flow entirely stops, preventing the attacker from penetrating the network.

\section{Overall Analysis}
\label{sec:results_overall}
\par Chapter \ref{chp:methodology} presents several questions that this research attempts to answer. Each is revisited below along with an accompanying conclusion. More details on each answer may be found in earlier sections of this chapter. 

\begin{enumerate}
\item Does \ac{ARG} classify traffic correctly?
\par Overall, \ac{ARG} does classify traffic correctly. Throughout the testing, no actual false negatives occurred, indicating \ac{ARG}'s \ac{NAT} system is robust and provides a solid way to classify invalid inbound traffic. However, the traffic used to test this system is fairly benign and does not intentionally attempt to circumvent \ac{ARG}. Testing needs to be done with a more intelligent threat. 

\par For expected traffic on the network, whether it is between \ac{ARG}-protected networks or to external hosts, \ac{ARG} is reasonably accurate. In basic tests involving a range of traffic types, \ac{ARG} averaged less than \Sexpr{basic.boot.overall.stat}\% loss of valid, expected traffic. This falls well below Chapter \ref{chp:methodology}'s definition of ``acceptable loss'' of \Sexpr{acceptable}\%. Based on this, \ac{ARG} accurately classifies traffic.

\item What is the maximum packet rate \ac{ARG} can handle?
\par The testing done for this research did not find an upper limit on the amount of traffic \ac{ARG} could handle, with the gateways showing the capability of handling over \Sexpr{round(max_packet_rate/1024, 1)} \ac{Mbps} with no statistically significant change in packet loss. 

\item What is the minimum supportable time between hops? How does latency affect this?
\par When \ac{ARG} operates on a network with a one-way latency less than 15 milliseconds, hops may occur every 50 to 75 milliseconds and still maintain a viable communication channel (losses are less than \Sexpr{acceptable}\%). Beyond this point, packets begin flowing smoothly when the time between hops exceeds four times the latency. However, there is reason to believe hops could be faster in a real-world deployment, due to the test network factors Section \ref{sec:results_hoprate_tests} discusses. This is a significant contribution because previous research focuses on address changes on the order of minutes or hours, rather than multiple times a second. 

\par Realistically, network latencies vary wildly. However, \ac{ARG} allows flexibility in setting the hop interval for every gateway separately and could be easily expanded to allow gateways to change the hop intervals they use between each other while leaving the rate used for others alone. Section \ref{sec:future_work} discusses this in more depth. This flexibility allows networks with low-latency networks to hop frequently and high-latency networks to hop more slowly. The results here provide guidance on the maximum hop interval to chose based on network conditions.

\item Is \ac{ARG} stable when presented with corrupt, malformed, or replayed packets?
\par \ac{ARG} is stable in the face of bad traffic. Long-running fuzz tests do not cause \ac{ARG} to crash, nor do they permanently break traffic flow. However, malformed traffic does appear to have an impact on valid packet loss, based on both automated analysis and a manual inspection. The cause of this needs exploration to determine the correct fix.
\end{enumerate}

\section{Summary}
\par This chapter analyzes the results for each test (Figure \ref{fig:testnum_flows}) sequence, each broken out into its own section. An overall analysis in context of the original research questions is then presented, summarizing the findings of this thesis.

