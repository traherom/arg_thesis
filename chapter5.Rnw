\chapter{Results and Analysis}
\label{chp:results}
\par This chapter presents and analyzes the experimental results. Each test sequence is discussed in a separate section, with Section \ref{sec:results_basic_tests} covering the basic functionality tests, Section \ref{sec:results_hoprate_tests} determining the effects of hop rate and latency, and Section \ref{sec:results_packetrate_tests} discussing \ac{ARG}'s performance. Section \ref{sec:results_fuzzer_tests} covers the results of running a fuzzer against \ac{ARG}. Finally, Section \ref{sec:results_overall} revists the research questions posed in Chapter \ref{chp:methodology} and summarizes the results with respect to each of them.

<<echo=FALSE, message=FALSE>>=
library(gplots)
library(plotrix)
library(boot)
library(coin)
library(cluster)
options(scipen=10)
@

<<echo=FALSE>>=
alpha=.95
reasonable=2
replicates=1000

# Bring in CSV file and break out into each test, excluding the results.dir and
# label columns
results = read.csv('results.csv', header=TRUE)
results = results[,1:ncol(results)-1] # Remove blank column at end

# Proportions are small and silly
loss_cols = c(colnames(results)[startsWith(colnames(results), 'valid.loss.rate')], 'invalid.loss.rate')
results[,loss_cols] = results[,loss_cols] * 100

# Tests were 0-indexed. Make them 1-indexed for readability
#results[,'test'] = results[,'test'] + 1

# Convert ms values to integeres
results[,'hop.rate'] = as.integer(sapply(results[,'gatea.hop.rate'], function(x) {
		x = as.character(x);
		substr(x, 1, nchar(x)-2);
	}))

# Combine stats to make them easy to access
results[,'kbps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.kbps'], x['gateb.kbps'], x['gatec.kbps'])
	)) })
results[,'pps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.pps'], x['gateb.pps'], x['gatec.pps'])
	)) })

results = split(results, results['label'])
basic = results$basic
hoprate = results$hoprate
packetrate = results$packetrate
fuzzer = results$fuzzer
@

\section{Basic Tests}
\label{sec:results_basic_tests}
\par The first set of tests run tests the basic functioning of \ac{ARG}, as discussed in Section \ref{sec:exp_design}. These tests are intended to validate the basic functionality of \ac{ARG} and demonstrate the accuracy of classification under various circumstances, so relatively packet rates are kept low. Hop rates vary between 50 milliseconds and 500 milliseconds, with round-trip latency fixed at 20 milliseconds.

<<echo=FALSE>>=
bootstrap_stats = function (data, idx) {
	m = mean(data[idx], na.rm=TRUE)
	return(m)
}

# Calculate overall average
basic.boot.overall = boot(data=basic[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
basic.boot.overall.stat = basic.boot.overall$t0
basic.boot.overall.ci = boot.ci(basic.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
basic.boot = by(basic[,'valid.loss.rate'], basic[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
basic.boot.stats = sapply(basic.boot, function(x) x$t0)
basic.boot.ci = sapply(basic.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
boot.t.test = function(x, y, R, n=10, conf=.95) {
	# Create replicates
	x_resampled = list()
	y_resampled = list()
	for(i in 1:R) {
		x_resampled[[i]] = sample(x, n)
		y_resampled[[i]] = sample(y, n)
	}

	# Test
	t = c()
	for(i in 1:R) {
		t = c(t, t.test(x_resampled[[i]], y_resampled[[i]], conf.level=conf)$statistic)
	}

	# Confidence intervals
	t = sort(t)
	cutoff = floor((1-conf) * length(t))
	return(c(t[cutoff], t[length(t) - cutoff]))
}

basic.hr = split(basic, basic[,'hop.rate'])
basic.hr.t = boot.t.test(basic.hr$'50'[,'valid.loss.rate'], basic.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
basic.hr.pvalue = pvalue(oneway_test(basic[,'valid.loss.rate']~factor(basic[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

# Utility to show why packets were rejected
create_rejection_table = function(name, caption, table) {
	cat('\\begin{table}[H]\n')
	cat('\\caption{')
	cat(caption)
	cat('}\n')
	cat('\\label{tbl:')
	cat(name)
	cat('}\n')
	cat('\\centering\n')
	cat('\\begin{tabular}{l|c|c}\n')
	cat('Reason & Count & \\% of Total Packets\\\\\n')
	cat('\\hline\n')

	# Total number of packets
	total_packets = sum(table[,'valid.sent'], table[,'invalid.sent'])

	# Sum just the rejection reason columns
	reasons = table[, sapply(colnames(table), function(x) {
			return(startsWith(x, 'inbound') || startsWith(x, 'outbound')
					|| x == 'unknown' || x == 'lost.on.wire')
		})]
	reasons = apply(reasons, 2, function(x) sum(x, na.rm=TRUE))
	reasons = sort(reasons, decreasing=TRUE)

	for(i in 1:length(reasons)) {
		if(is.na(reasons[i]) || reasons[i] == 0)
			next
		
		r = names(reasons)[i]
		r = gsub('.', ' ', r, fixed=TRUE)
		r = paste(toupper(substr(r, 1, 1)), substring(r, 2), sep='')

		cat(r, ' & ', reasons[i], ' & ', reasons[i]/total_packets*100, '\\%', sep='')
		if(i < length(reasons))
			cat('\\\\\n')
	}

	cat('\\end{tabular}\n')
	cat('\\end{table}\n')
}
@

\subsection{Valid Packet Loss}
\label{sec:basic_valid_packet_loss}
\par The raw results for the loss of valid packets on each test are shown in Figure \ref{fig:basic_raw_data}. Figure \ref{fig:basic_mean_ci} shows the mean and \acp{CI} of each test, using a \Sexpr{alpha*100}\% confidence level. Due to a lack of normality in the experimental results, these numbers are calculated via bootstrapping with \Sexpr{replicates} replicates.

\begin{figure}
\caption{Basic Test Data}
\centering
<<echo=FALSE>>=
# Graph and generate stats on basic tests with test num vs loss rate 
ylim=c(0, max(basic[,'valid.loss.rate'], na.rm=TRUE))

plot(basic[,'test'], basic[,'valid.loss.rate'],
	xlab="Test Number", xaxt='n',
	ylab="Valid Packet Loss", ylim=ylim, yaxt='n')
axis(1, at=names(basic.boot), labels=names(basic.boot))
ylab = seq(0, max(ylim), .5)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\caption{Raw Data}
\label{fig:basic_raw_data}
\end{figure}

\begin{figure}
\centering
<< echo=FALSE, warning=FALSE>>=
ylim=c(0, max(basic.boot.ci, na.rm=TRUE))

plot.new()
plot.window(xlim=c(0, length(basic.boot)-1), ylim=ylim)
title(main="", xlab="Test Number", ylab="Mean Valid Packet Loss")

axis(1, at=names(basic.boot), labels=names(basic.boot))
ylab = seq(0, max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
box()

points(0:(length(basic.boot)-1), basic.boot.stats)
plotCI(0:(length(basic.boot)-1), basic.boot.stats,
	ui=sapply(basic.boot.ci[2,], function(x) min(x, 1)),
	li=sapply(basic.boot.ci[1,], function(x) max(x, 0)),
	add=TRUE, scol='blue')

#text(0:(length(basic.boot)-1) + .5, basic.boot.stats,
#	labels=sapply(basic.boot.stats, function(x) round(x, digits=4)))
@
\caption{Mean and \ac{CI}}
\label{fig:basic_mean_ci}
\end{figure}

\par These figures reveal a low packet loss across all test scenarios. At worst, Test 3---TCP traffic between \ac{ARG} protected clients only---lost between \Sexpr{basic.boot.ci[1,4]}\% and \Sexpr{basic.boot.ci[2,4]}\% of packets, with \Sexpr{alpha*100}\% confidence. Across all tests, valid packet loss averaged \Sexpr{basic.boot.overall.stat}\%, with a \ac{CI} of $(\Sexpr{I(basic.boot.overall.ci)})$, again with \Sexpr{alpha*100}\% confidence. These results confirm Chapter \ref{chp:methodology}'s hypothesis that \ac{ARG} causes packet loss less than \Sexpr{reasonable}\%, at least under normal conditions.

\par Figure \ref{fig:basic_raw_data} displays two outliers on Test 3 and an occasional outlier in Test 5, 6, and 7. Examining the data reveals the dropped packets exceeded the maximum size of a packet \ac{ARG} can pass between gateways. As Chapter \ref{chp:implementation} covers, \ac{ARG} wraps packets between gateways in its own headers, increasing the size of the packet. Ethernet II has a default maximum transmission unit of 1500 bytes, so \ac{ARG} drops packets that pass this limit after being wrapped (\ac{ARG} is unable to fragment packets). In fact, the maximum packet size issue is the most common reason for packet rejection in the basic set of tests (in tests with only valid traffic), as shown in Table \ref{tbl:basic_loss_reasons_valid}. While test traffic is controlled for size, the test traffic generators fail to account for \ac{TCP} options, which can increase the size of the packet beyond the limit.

<<echo=FALSE, results='asis'>>=
valid_runs = apply(basic, 1, function(x) x['test'] < 5)
num_runs = nrow(basic[valid_runs,])
create_rejection_table('basic_loss_reasons_valid', 'Basic Tests 1-4 Packet Reject Reasons', basic[valid_runs,])
@

\par Other entries in this table deserve some disscussion. The next most common reason, incorrect sequence numbers, indicates that \ac{ARG}'s replay protection mechanism blocked packets. This commonly occurs at initialization, which with \Sexpr{num_runs} runs averages out to around only three sequence number problems per test. The third entry on the list, ``Inbound Unwrapped,'' occurs at the end of a test, where the receiver of a packet is shutdown before the sender (this is also the case for ``outbound rewrite'' and ``ping accepted''). The gateway unwraps the packet, but the test processor is unable to find the receive, causing it to return the last message it saw regarding the packet. Finally, ``unknown'' indicates the processor traced the packet to a point where it disappeared via traffic captures, but found no message in the receiver's logs indicating what it did with the packet. The remainder of the reasons are self explanatory.

\par A resampling of the data between the two hop rates used in the basic tests gives a p-value of \Sexpr{round(basic.hr.pvalue, 3)}, indicating little significant difference between the two hop rates. This confirms that hop rate in and of itself does not affect losses on the network, an important fact when considering the maximum hop rate in Section \ref{sec:results_hoprate_tests}. %convincing evidence that there is a statistically significant difference introduced by the different hop rates. However, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms, just barely twice the 20 ms latency). Ideally these tests would be re-run with a hop rate closer to 100 milliseconds to avoid interaction with the latency.

%However, the \ac{CI} for this difference is a mere $(\Sexpr{I(ci_hr)})$ at a \Sexpr{alpha*100}\% confidence level, fairly small for the purposes of network traffic. Additionally, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms with a latency of 20 ms).

\FloatBarrier
\subsection{Invalid Loss Rate}
\par Tests 5 through 8 include invalid traffic that \ac{ARG} should reject and hence should be ``lost.'' Figure \ref{fig:basic_invalid_loss} displays the percentage of invalid traffic rejected (with 95\% \acp{CI}), clearly revealing \ac{ARG} has an extremely low false negative rate. As shown, over the course of nearly 30 repetitions only one test appears to have allowed a single packet through. %\Sexpr{alpha*100}\% \ac{CI} are theoretically displayed, but all showed 100\% invalid packet loss.

\begin{figure}
\caption{Invalid Traffic Loss Rate}
\label{fig:basic_invalid_loss}
\centering
<<basic_invalid_loss, out.width=".75\\textwidth", echo=FALSE, warning=FALSE>>=
# Show loss rate of invalid traffic for tests 5-8.
invalid_runs = !valid_runs
ylim = c(99.9, 100)
plotmeans(basic[invalid_runs, 'invalid.loss.rate']~basic[invalid_runs, 'test'], p=alpha,
		connect=FALSE,
		minbar=0, maxbar=100, ylim=ylim, yaxt='n',
		xlab="Test Number", ylab="Invalid Packet Loss")
ylab = seq(0, max(ylim), .01)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}

\par Test 8 shows a small deviation from the expected 100\% packet rejection, as 1 of 746 packets made it through, but further examination of this number shows a rare post-processing problem. If two identical packets are sent at the same time, the test run processor leaves the second one unmarked, which is interpreted later as successfully received. The problem lies with the log analyzer, not \ac{ARG} itself, which did in fact reject every packet. This result offers convincing evidence that \ac{ARG} effectively blocks unexpected inbound traffic, but it gives no indication of its suitablity against more focused attacks. 

<<echo=FALSE, results='asis'>>=
# Get the total for each possible rejection reason for the invalid tests only
create_rejection_table('basic_loss_reasons_invalid', 'Basic Tests 5-8 Packet Rejection Reasons', basic[invalid_runs,])
@

\par Additionally, Table \ref{tbl:basic_loss_reasons_invalid} illustrates that packets are rejected via the \ac{NAT} table quite frequently. This operation costs the gateway minimal processing time, as it can reject after a single hashtable lookup. The less time wasted on invalid packets the better, as it lowers the possibility of a \ac{DOS} attack; if all bandwith is consumed there is still a problem, but that is beyond the control of \ac{ARG}. Section \ref{sec:basic_valid_packet_loss} discusses the meaning of the remainder of this table.

\FloatBarrier
\section{Maximum Hop Rate}
\label{sec:results_hoprate_tests}
\par The maximum hop rate sequence of tests measures the change in packet loss rate at specific latencies as hop rate increases. For these tests, a fixed packet rate and Test 4 is always used. Section \ref{sec:exp_design} covers the specifics of these tests. The figures below document the results of the tests, broken up by traffic type and direction.

<<echo=FALSE>>=
# Change latency to one-way (currently RTT)
hoprate[,'latency'] = hoprate[,'latency'] / 2

plot_latency_hoprate = function(hoprate, loss_rate_col, title='', xlim=NULL, ylim=NULL, yby=2) {
	# Get mean of each hop rate-latency pair. Matrix
	# rows are latencies, columns are hop rates
	lat_hr_mean = matrix(nrow=length(unique(hoprate[,'latency'])),
						ncol=length(unique(hoprate[,'hop.rate'])))
	rownames(lat_hr_mean) = sort(unique(hoprate[,'latency']))
	colnames(lat_hr_mean) = sort(unique(hoprate[,'hop.rate']))

	worst_sd = 0
	repetitions = c()
	for(lat in rownames(lat_hr_mean)) {
		for(hr in colnames(lat_hr_mean)) {
			rows = hoprate[,'latency'] == lat & hoprate[,'hop.rate'] == hr

			# Calculate mean for each test number
			#b = boot(data=hoprate[rows,], statistic=function(d, i) {
			#			return(c(mean(d[i], na.rm=TRUE), sd(d[i], na.rm=TRUE)))
			#		}, R=replicates)

			lat_hr_mean[lat, hr] = mean(hoprate[rows, loss_rate_col])
			#lat_hr_mean[lat, hr] = b$t0

			repetitions = append(repetitions, length(hoprate[rows, loss_rate_col]))

			curr_sd = sd(hoprate[rows, loss_rate_col])
			#curr_sd = b$t1
			if(!is.na(curr_sd) && curr_sd > worst_sd)
				worst_sd = curr_sd
		}
	}

	# Plot. Each latency is separate, x is hop rate, y is loss
	old_par = par(no.readonly=TRUE)
	par(mar=c(5.1, 4.1, 4.1, 7.8))

	plot.new()
	if(is.null(xlim))
		xlim = c(1, ncol(lat_hr_mean))
	if(is.null(ylim))
		ylim = c(0, max(lat_hr_mean, na.rm=TRUE))
	plot.window(xlim=xlim, ylim=ylim)
	title(main=title, xlab="Hop Interval (ms)", ylab="Packet Loss")

	axis(1, at=1:ncol(lat_hr_mean), labels=colnames(lat_hr_mean), cex.axis=0.75)
	yaxis = c(reasonable, seq(from=min(ylim), to=max(ylim), by=yby))
	axis(2, at=yaxis, labels=paste(yaxis, '%', sep=''))

	box()

	abline(h=reasonable, lty=5, col="purple")

	type=1
	for(lat in rownames(lat_hr_mean)) {
		lines(lat_hr_mean[lat,], col=type, lty=type)
		points(lat_hr_mean[lat,], col=type, pch=type-1)
		type = type + 1
	}

	legend(title="Latency",
			xpd=TRUE,
			x=xlim[2] + .6,
			y=mean(ylim) + (ylim[2] - ylim[1])/5,
			legend=c(paste(rownames(lat_hr_mean), "ms"), paste('Reasonable\nLoss: ', reasonable, '%', sep='')),
			lty=c(1:(type-1), 5),
			col=c(1:(type-1), "purple"),
			pch=c(0:(type-2), NA)
			)

	par(old_par)

	return(c(sd=worst_sd, repetitions=mean(repetitions)))
}
@

\begin{figure}
\caption{Loss Rate, \ac{ARG} Network Traffic}
\label{fig:hrlat_udptcp_inter_full}
\centering
<<echo=FALSE>>=
plot_results = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', yby=5)
worst_sd = plot_results['sd']
repetitions = plot_results['repetitions']
@
\end{figure}

\begin{figure}
\caption{Loss Rate, \ac{ARG} Network Traffic, Scaled}
\label{fig:hrlat_udptcp_inter_zoomed}
\centering
<<echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', ylim=c(0, 5), yby=.5)
@
\end{figure}

\par Figure \ref{fig:hrlat_udptcp_inter_full} displays the results of the hop rate tests, separated by network latency. Figure \ref{fig:hrlat_udptcp_inter_zoomed} displays the same numbers, but with a much tighter Y scale to reveal differences at large hop intervals. The numbers shown here only include packets sent between \ac{ARG}-protected clients, traffic to the external host and administrative packets between gateways are not included. The values given are the bootstrapped means over \Sexpr{round(repetitions)} test repetitions with \Sexpr{replicates} replicates used in the bootstrap. \tbd{not true right now}. The worst standard deviation in any of the hop rate-latency groups is \Sexpr{worst_sd}, implying the potential for significant (but still potentially acceptable) variation in network performance. %implying some variation in each hop rate-latency grouping but still relatively predictable losses.

\par Interestingly, the zero millisecond latency trials exhibit a noticably different decline than the others, with zero milliseconds giving a relatively steady decline in loss rate and the others remaining relatively flat before a rapid drop. The 30 millisecond latency curve gives a hint the shapes may not be as different as it appears, as it shows two or three mid-way data points on the way down to reasonable packet loss. With tests against more hop rates, these curves would likely show a more gradual (although still rapid) decline. 

\par Chapter \ref{chp:methodology} hypothesizes that packet loss reaches reasonable levels when the hop rate is equal to (or exceeds) the one-way network latency. Theoretically, this time frame should allow a packet to cross the network from one gateway to another before the receiver changes \acp{IP} twice, even if the packet is sent just before the receiver hops the first time (as mentioned in Chapter \ref{chp:implementation}, gateways accept packets at the current and previous \ac{IP}). Any faster hop rate results in a period of time before each hop that guarrantees sent packets arrive too late.  Figure \ref{fig:hrlat_udptcp_inter_zoomed}, however, shows that loss drops below \Sexpr{reasonable}\% when hop rate is around four times the latency. This difference from the anticipated behavior is likely the result of the test environment.

\par As discussed in Section \ref{sec:eth_routing}, Ethernet uses \ac{ARP} requests and responses to transfer packets between machines. In the test environment, the gateways and the external host are all on the same local network and hence must work with all the extra Ethernet frames that entails.  On a normal Ethernet segment with minimal latency (the zero millisecond test), the \ac{ARP} process takes minimal time and has little impact on operation. To speed the process even further, \ac{ARP} data is cached, so when a system wants to send to the same \ac{IP} again, it references the ARP cache table and does not have to go through the request-response process again. With fast hop rates, however, gateways frequently need to send to different \ac{IP} addresses and therefore must send and receive the requiste \acp{ARP} very frequently (generally, once every hop). When the network latency is not zero, these additional frames can cause siginficant delays. Figure \ref{fig:triple_arp_issue} illustrates what happens when a time synchronization between gateways occurs with a five millisecond one-way latency.

\begin{figure}
\caption{Time Synchronization, Including \ac{ARP}s}
\label{fig:triple_arp_issue}
\centering
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue}
\caption{With 5 millsecond \ac{ARP}}
\end{subfigure}
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue_no_arp}
\caption{With 0 millsecond \ac{ARP} (not shown)}
\end{subfigure}
\end{figure}

\par If there were no need for these ARP requests (or they were negligible), then the entire time sync process would complete in 10 milliseconds. Because of the \acp{ARP}, however, it takes 30 milliseconds. The time base for the other gateway is still (usually) calculated correctly, because the latency of that particular packet is taken into account. Later packets may take a wide variety of latencies, however, because they may not require any \acp{ARP}, only one direction might require it, or both directions may need it. For a 5 millisecond one-way latency, this only leads to variantions of 5, 10, or 15 milliseconds, a tolerable amount of variation. By the time tests get to 250 milliseconds latencies, one-way latencies easily range from 250 to 750 milliseconds, with \ac{RTT} of around 1500 milliseconds. At faster hop rates, the triple latency problem is encountered more frequently, as the ARP cache must be refreshed more often. This degree of variation is difficult to compensate for, leading to siginificant packet loss until hop rates exceed three times the set latency.

\par Desite having no latency, the zero millisecond test exhibits high loss until around 75 milliseconds hops, similar to when the 15 millisecond latency test indicates a usable connection. This may indicate a maximum supportable hop rate of around 50-75 milliseconds, at least without the sender calculating \acp{IP} with respect to latency (i.e., send packets with \acp{IP} that will be current when they arrive at the receiver). Previous rsearch into address space randomization and \ac{IP} hopping have limited themselves to hopping on the order of minutes or hours \tbd{cite}.

\par \ac{ARG} appears to have little additional impact on \ac{TCP}, when compared to \ac{UDP}. Figure \ref{fig:hrlat_tcp_inter} displays losses associated with only \ac{TCP} packets travelling between gateways. Compared to Figure \ref{fig:hrlat_udptcp_inter_full} (which includes UDP and TCP packets), trends appear similar. \tbd{resample between tcp-only and full interarg}

\begin{figure}
\caption{Loss Rate, TCP Packets Only}
\label{fig:hrlat_tcp_inter}
\centering
<<echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.tcp.interarg', yby=5)
@
\end{figure}

<<echo=FALSE>>=

@

\par Table \ref{tbl:hoprate_loss_reasons} shows the packet rejection reasons for all hop rate tests. As expected, far and away the most common rejection reason for this set of tests is incorrect source and/or destination \acp{IP}. Other entries in this table are discussed in Section \ref{sec:basic_valid_packet_loss}.

<<echo=FALSE, results='asis'>>=
create_rejection_table('hoprate_loss_reasons', 'Hoprate Loss Reasons', hoprate)
@

\par Finally, Figure \ref{fig:hrlat_extra} demonstrates there is little correlation between hop rate and the flow of traffic to external hosts. In the worst sample of all \Sexpr{nrow(hoprate)} runs, packet loss to and from the external host reached only \Sexpr{signif(max(hoprate[,'valid.loss.rate.extraarg']), 2)}\%. This makes sense, as the changing \ac{IP} is only referenced when the \ac{NAT} module first creates a table entry and consults that information for all future packets on that connection. Barring genuine bugs in that process, the hop rate is completely irrelavant to performance to external hosts.

\begin{figure}
\caption{Loss Rate, Externally-Bound Packets Only}
\label{fig:hrlat_extra}
\centering
<<echo=FALSE>>=
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.extraarg', yby=.005)
@
\end{figure}

\section{Maximum Packet Rate}
\label{sec:results_packetrate_tests}
\par The final numerical test performed against \ac{ARG} tests the maximum packet rate it is capable of handling. For these tests, latency is set to 20 milliseconds and Test 4 is used. Hop rate varies between 50 ms and 500 ms to check if this has an effect on the supported packet rate. Most importantly, traffic generators are run with steadily increasing send rates. See Section \ref{sec:exp_design} for details. Because throughput varies somewhat on each run, the data is clustered into the groupings listed in Table \ref{tbl:pr_clusters} and colored in Figure \ref{fig:pr_raw_data}.

<<echo=FALSE>>=
# Sort base on throughput
packetrate = packetrate[order(packetrate[,'kbps']),]

# Cluster data by kbps
cols = c('kbps', 'valid.loss.rate')
nclusters = 7
packetrate.fit = kmeans(packetrate[,cols], nclusters, nstart=500)
packetrate[,'bps.clust'] = packetrate.fit$cluster

# Reorder cluster numbers to be increasing according to kbps
old_clusters = packetrate[,'bps.clust']
curr_cluster_start = 1
for(curr in 1:max(old_clusters)) {
	clust = old_clusters == old_clusters[curr_cluster_start]
	packetrate[clust,'bps.clust'] = curr
	curr_cluster_start = curr_cluster_start + sum(clust)
}

# Calculate mean kbps for each throughput cluster
packetrate.boot = by(packetrate[,'valid.loss.rate'], packetrate[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
packetrate.boot.stats = sapply(packetrate.boot, function(x) x$t0)
packetrate.boot.ci = sapply(packetrate.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='packetrate')$packetrate
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
packetrate.hr = split(packetrate, packetrate[,'hop.rate'])
packetrate.hr.t = boot.t.test(packetrate.hr$'50'[,'valid.loss.rate'], packetrate.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
packetrate.hr.pvalue = pvalue(oneway_test(packetrate[,'valid.loss.rate']~factor(packetrate[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

clust_colors = c("black", "red", "green", "purple",
				 "blue", "magenta", "cyan")
@

\begin{table}
\caption{Packet Rate Clusters}
\label{tbl:pr_clusters}
\centering
\begin{tabular}{c|c|c|c}
Num & Mean (Kbps) & Min (Kbps) & Max (Kbps)\\
\hline
<<echo=FALSE, results='asis'>>=
for(clust in unique(packetrate[,'bps.clust'])) {
	kbps = packetrate[packetrate[,'bps.clust'] == clust,'kbps']
	cat(clust)
	cat(' & ')
	cat(mean(kbps))
	cat(' & ')
	cat(min(kbps))
	cat(' & ')
	cat(max(kbps))
	cat('\\\\\n')
}
@
\end{tabular}
\end{table}

\begin{figure}
\caption{Loss verses throughput, raw packet rate data with clustering}
\label{fig:pr_raw_data}
\centering
<<echo=FALSE>>=
# Plot 
ylim = c(0, .4)

plot(packetrate[,'kbps'], packetrate[,'valid.loss.rate'],
	ylim=ylim, yaxt='n',
	col=clust_colors[packetrate[,'bps.clust']],
	pch=packetrate[,'bps.clust'] - 1,
	xlab="Throughput (Kbps)", ylab="Valid Packet Loss",
	sub="Colors indicate clustering")
for(clust in unique(packetrate[,'bps.clust'])) {
	rows = packetrate[,'bps.clust'] == clust
	lines(packetrate[rows,'kbps'], packetrate[rows,'valid.loss.rate'], col=clust_colors[clust])

	# Divider line between each with number of cluster
	kbps_curr = packetrate[packetrate[,'bps.clust'] == clust,'kbps']
	kbps_next = packetrate[packetrate[,'bps.clust'] == clust + 1,'kbps']
	if(clust < nclusters) {
		abline(v=mean(c(max(kbps_curr), min(kbps_next))), lty=3)
	}

	# Label each cluster
	text(mean(kbps_curr), 0, clust, col=clust_colors[clust]) 
}

ylab = seq(min(ylim), max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))

max_packet_rate = max(packetrate[,'kbps'])
@
\end{figure}

\par Figure \ref{fig:pr_raw_data} shows the raw data from these tests, with colors indicating the \Sexpr{max(packetrate[,'bps.clust'])} K-means clusters, grouped by bit-wise throughput. This graph includes all packets on the network. Visually, the plot reveals fairly similar loss in each cluster, although some clusters exhibit a fair amount of variation. A Tukey test with each cluster confirms this result, as shown in \ref{fig:pr_tukey}. 

\begin{figure}
\caption{Tukey Test Against Clustered Packet Rate Data}
\label{fig:pr_tukey}
<<echo=FALSE>>=
m = aov(valid.loss.rate~factor(bps.clust), packetrate)
t = TukeyHSD(m, conf.level=alpha)
plot(t)
@
\end{figure}

\par On this graph, numbers along the side correspond to the cluster numbers shown in Table \ref{tbl:pr_clusters} and Figure \ref{fig:pr_raw_data}. Six means stand out as being distinctly different: 1-4, 2-4, 3-4, 4-5, 4-6, and 4-7. Cluster 4 is statistically different from all other clusters, with \Sexpr{alpha*100}\% confidence. However, the other data gives reason to believe that throughput was not the cause of loss increase and variation. No changing trend is seen in any of the surrounding throughputs' losses (either up or down) and faster throughputs (Clusters 5 through 7) show loss comparable to the slower throughputs (Clusters 1 through 3). While the precise cause of the increased loss in some of these tests is unknown, the amount of data the gateways handle is not believed to be the cause.

\par The results of these tests are limited to just the implementation of \ac{ARG} and imply only a minimally possible throughput. Despite these limitations, \ac{ARG} performs well with reasonably rapid traffic of over four \ac{Mbps}, giving no reason to believe it is not capable of higher rates.

\section{Fuzzing Test}
\label{sec:results_fuzzer_tests}
\par The final series of tests run on \ac{ARG} consist of throwing invalid and replayed traffic against the gateways. The primary objective is to verify that \ac{ARG} remains stable and able to pass valid traffic while under attack. 

<<echo=FALSE>>=
# Calculate overall average
fuzzer.boot.overall = boot(data=fuzzer[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
fuzzer.boot.overall.stat = fuzzer.boot.overall$t0
fuzzer.boot.overall.ci = boot.ci(fuzzer.boot.overall, conf=alpha, type='fuzzer')$fuzzer[4:5]

# Calculate mean for each test number
fuzzer.boot = by(fuzzer[,'valid.loss.rate'], fuzzer[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
fuzzer.boot.stats = sapply(fuzzer.boot, function(x) x$t0)
fuzzer.boot.ci = sapply(fuzzer.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='fuzzer')$fuzzer
	return(ci[4:5])
	})
@

\par Unfortunately, the results processor this thesis uses to generate statistics on runs is unable to process the logs from the malicious traffic generators and the malformed data in the pcap files is unusable, from its perspective. This makes it difficult to validate the accuracy of results from the runs. Nonetheless, the \Sexpr{signif(fuzzer.boot.overall.stat, 2)}\% lost over each run indicates \ac{ARG} experiences significant difficulty when faced with malicious traffic. A manual examination of log files does seem to indicate this is the case, with some traffic successfully traversing across the network but a huge amount being rejected. 

\begin{comment}
Table \ref{tbl:fuzzer_loss_reasons} gives the reasons behind each rejection.

<<results='asis', echo=FALSE>>=
create_rejection_table('fuzzer_loss_reasons', 'Fuzzer Packet Reject Reasons', fuzzer)
@
\end{comment}

\par Despite the high losses, \ac{ARG} does appear to remain stable throughout the barrage. In all cases, \ac{ARG} never crashes and maintains around the same ability to handle valid traffic (that is, although the losses appear high, they remain consistent throughout). Further research is needed to identify the causes behind the high losses, but the problem likely lies in implementation and not architecture.

\section{Overall Analysis}
\label{sec:results_overall}
\par Chapter \ref{chp:methodology} presents several questions that this research attempts to answer. Each is revisited below along with an accompanying conclusion. More details on each answer may be found in earlier sections of this chapter. 

\begin{enumerate}
\item Does \ac{ARG} classify traffic correctly?
\par Overall, \ac{ARG} does classify traffic correctly. Throughout the testing, no actual false negatives occured, indicating \ac{ARG}'s \ac{NAT} system is robust and provides a solid way to classify invalid inbound traffic. However, the traffic used to test this system is fairly benign and does not intentionally attemp to circumvent \ac{ARG}. Testing needs to be done with a more intelligent threat. 

\par For expected traffic on the network, whether it is between \ac{ARG}-protected networks or to external hosts, \ac{ARG} is reasonably accurate. In basic tests involving a range of traffic types, \ac{ARG} averaged less than \Sexpr{basic.boot.overall.stat}\% loss of valid, expected traffic. This falls well below Chapter \ref{chp:methodology}'s definition of ``reasonable loss'' of \Sexpr{reasonable}\%. Based on this, \ac{ARG} accurately classifies traffic.

\item What is the max packet rate \ac{ARG} can handle?
\par The testing done for this research did not find an upper limit on the amount of traffic \ac{ARG} could handle, with the gateways showing the capability of handling over \Sexpr{round(max_packet_rate, 1)} \ac{Kbps} with little change in loss rate. While a system deployed in front of a corporate network would need to handle much higher rates, these tests find no reason to believe \ac{ARG} incapable of scaling well.

\item What is the maximum supportable hop rate? How does latency affect this?
\par When \ac{ARG} operates on a network with a one-way latency less than 15 milliseconds, hops may occur every 50 to 75 milliseconds and still maintain a viable communication channel (losses are less than \Sexpr{reasonable}\%). Beyond this point, packets begin flowing smoothly when the time between hops exceeds four times the latency. However, there is reason to believe hops could be faster in a real-world deployment, due to test network factors. See Section \ref{sec:results_hoprate_tests} for details on this. This is a sigificant contribution because previous research focuses on address changes on the order of minutes or hours, rather than multiple times a second. \tbd{cite}

\item Is \ac{ARG} stable when presented with corrupt, malformed, or replayed packets?
\par \ac{ARG} is stable in the face of bad traffic. Long-running fuzz tests do not cause \ac{ARG} to crash, nor do they permanently break traffic flow. However, \tbd{need data}
\end{enumerate}

\section{Summary}
\par This chapter analysizes the results for each test sequence, each broken out into its own section. An overall analysis in context of the original research questions is then presented, summarizing the findings of this thesis.

