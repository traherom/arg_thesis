\chapter{Results and Analysis}
\label{chp:results}
\par This chapter presents and analyzes the experimental results. Each test sequence is discussed in a separate section, with Section \ref{sec:results_basic_tests} covering the basic functionality tests, Section \ref{sec:results_hoprate_tests} determining the effects of hop rate and latency, and Section \ref{sec:results_packetrate_tests} discussing \ac{ARG}'s performance. Finally, Section \ref{sec:results_fuzzer_tests} covers the results of running a fuzzer against \ac{ARG}.

<<echo=FALSE, message=FALSE>>=
library(gplots)
library(plotrix)
library(boot)
library(coin)
options(scipen=10)
@

<<echo=FALSE>>=
alpha=.95
reasonable=2
replicates=1000

# Bring in CSV file and break out into each test, excluding the results.dir and
# label columns
results = read.csv('results.csv', header=TRUE)
results = results[,1:ncol(results)-1] # Remove blank column at end

# Proportions are small and silly
results[,'valid.loss.rate'] = results[,'valid.loss.rate'] * 100
results[,'invalid.loss.rate'] = results[,'invalid.loss.rate'] * 100

# Tests were 0-indexed. Make them 1-indexed for readability
#results[,'test'] = results[,'test'] + 1

# Convert ms values to integeres
results[,'hop.rate'] = as.integer(sapply(results[,'gatea.hop.rate'], function(x) {
		x = as.character(x);
		substr(x, 1, nchar(x)-2);
	}))

# Combine stats to make them easy to access
results[,'kbps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.kbps'], x['gateb.kbps'], x['gatec.kbps'])
	)) })
results[,'pps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.pps'], x['gateb.pps'], x['gatec.pps'])
	)) })

results = split(results, results['label'])
basic = results$basic
hoprate = results$hoprate
packetrate = results$packetrate
fuzzer = results$fuzzer
@

\section{Basic Tests}
\label{sec:results_basic_tests}
\par The first set of tests run tests the basic functioning of \ac{ARG}, as discussed in Section \ref{sec:exp_design}. These tests are intended to validate the basic functionality of \ac{ARG} and demonstrate the accuracy of classification under various circumstances.

<<echo=FALSE>>=
bootstrap_stats = function (data, idx) {
	m = mean(data[idx], na.rm=TRUE)
	return(m)
}

# Calculate overall average
basic.boot.overall = boot(data=basic[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
basic.boot.overall.stat = basic.boot.overall$t0
basic.boot.overall.ci = boot.ci(basic.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
basic.boot = by(basic[,'valid.loss.rate'], basic[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
basic.boot.stats = sapply(basic.boot, function(x) x$t0)
basic.boot.ci = sapply(basic.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
boot.t.test = function(x, y, R, n=10, conf=.95) {
	# Create replicates
	x_resampled = list()
	y_resampled = list()
	for(i in 1:R) {
		x_resampled[[i]] = sample(x, n)
		y_resampled[[i]] = sample(y, n)
	}

	# Test
	t = c()
	for(i in 1:R) {
		t = c(t, t.test(x_resampled[[i]], y_resampled[[i]], conf.level=conf)$statistic)
	}

	# Confidence intervals
	t = sort(t)
	cutoff = floor((1-conf) * length(t))
	return(c(t[cutoff], t[length(t) - cutoff]))
}

basic.hr = split(basic, basic[,'hop.rate'])
basic.hr.t = boot.t.test(basic.hr$'50'[,'valid.loss.rate'], basic.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
basic.hr.pvalue = pvalue(oneway_test(basic[,'valid.loss.rate']~factor(basic[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

# Utility to show why packets were rejected
create_rejection_table = function(name, caption, table) {
	cat('\\begin{table}\n')
	cat('\\caption{')
	cat(caption)
	cat('}\n')
	cat('\\label{tbl:')
	cat(name)
	cat('}\n')
	cat('\\centering\n')
	cat('\\begin{tabular}{l|c|c}\n')
	cat('Reason & Count & Percentage of Total\\\\\n')
	cat('\\hline\n')

	# Total number of packets
	total_packets = sum(table[,'valid.sent'], table[,'invalid.sent'])

	# Sum just the rejection reason columns
	reasons = table[, sapply(colnames(table), function(x) {
			return(startsWith(x, 'inbound') || startsWith(x, 'outbound')
					|| x == 'unknown' || x == 'lost.on.wire')
		})]
	reasons = apply(reasons, 2, function(x) sum(x, na.rm=TRUE))
	reasons = sort(reasons, decreasing=TRUE)

	for(i in 1:length(reasons)) {
		if(is.na(reasons[i]) || reasons[i] == 0)
			next
		
		r = names(reasons)[i]
		r = gsub('.', ' ', r, fixed=TRUE)
		r = paste(toupper(substr(r, 1, 1)), substring(r, 2), sep='')

		cat(r, ' & ', reasons[i], ' & ', reasons[i]/total_packets*100, '\\%', sep='')
		if(i < length(reasons))
			cat('\\\\\n')
	}

	cat('\\end{tabular}\n')
	cat('\\end{table}\n')
}
@

\subsection{Valid Loss Rate}
\par The raw results for the loss of valid packets on each test are shown in Figure \ref{fig:basic_raw_data}. \tbd{how many replications each test has?}. Figure \ref{fig:basic_mean_ci} shows the mean and \acp{CI} of each test, using a \Sexpr{alpha*100}\% confidence level. Due to a lack of normality in the experimental results, these numbers are calculated via bootstrapping with \Sexpr{replicates} replicates.

\begin{figure}
\caption{Basic Test Data}
\begin{subfigure}[t]{0.5\linewidth}
\centering
<<basic_raw_data, echo=FALSE>>=
# Graph and generate stats on basic tests with test num vs loss rate 
ylim=c(0, max(basic[,'valid.loss.rate'], na.rm=TRUE))

plot(basic[,'test'], basic[,'valid.loss.rate'],
	xlab="Test Number", xaxt='n',
	ylab="Valid Packet Loss Rate (Percentage)", ylim=ylim)
axis(1, at=names(basic.boot), labels=names(basic.boot))
@
\caption{Raw Data \tbd{add \% to numbers. Talk about outlier}}
\label{fig:basic_raw_data}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}[t]{0.5\linewidth}
\centering
<<basic_mean_ci, echo=FALSE, warning=FALSE>>=
plot.new()
plot.window(xlim=c(0, length(basic.boot)-1), ylim=ylim)
title(main="", xlab="Test Number", ylab="Valid Packet Loss Rate (Percentage)")
axis(1, at=names(basic.boot), labels=names(basic.boot))
axis(2)
box()

points(0:(length(basic.boot)-1), basic.boot.stats)
plotCI(0:(length(basic.boot)-1), basic.boot.stats,
	ui=sapply(basic.boot.ci[2,], function(x) min(x, 1)),
	li=sapply(basic.boot.ci[1,], function(x) max(x, 0)),
	add=TRUE, scol='blue')

text(0:(length(basic.boot)-1) + .5, basic.boot.stats,
	labels=sapply(basic.boot.stats, function(x) round(x, digits=4)))
@
\caption{Mean and \ac{CI} \tbd{add \% to numbers}}
\label{fig:basic_mean_ci}
\end{subfigure}
\end{figure}

\par These figures reveal a low packet loss rate across all test scenarios. At worst, test 3---\ac{TCP} only with traffic to the external host---lost between \Sexpr{basic.boot.ci[1,4]}\% and \Sexpr{basic.boot.ci[2,4]}\% of packets, with \Sexpr{alpha*100}\% confidence. Across all tests, valid packet loss averaged \Sexpr{basic.boot.overall.stat}\%, with a \ac{CI} of $(\Sexpr{I(basic.boot.overall.ci)})$, again with \Sexpr{alpha*100}\% confidence. These results confirm Chapter \ref{chp:methodology}'s hypothesis that \ac{ARG} causes packet loss less than \Sexpr{reasonable}\%, at least under normal conditions.

\par A resampling of the data between the two hop rates used in the basic tests gives a p-value of \Sexpr{basic.hr.pvalue}, evidence that there is a statistically siginificant difference introduced by the different hop rates. However, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms with a latency of 20 ms). Ideally these tests would be re-run with a hop rate closer to 100 ms to avoid interaction with the latency.

\begin{comment}
<<echo=FALSE, results='asis'>>=
valid_runs = apply(basic, 1, function(x) x['test'] < 5)
create_rejection_table('basic_loss_reasons_all', 'Basic Tests 1-4 Packet Reject Reasons', basic[valid_runs,])
@
\end{comment}

\tbd{ideally, tests would be rerun with a hop rate of 100 ms or something, to avoid that bad interaction. I was clearly not thinking when I did that.}

%However, the \ac{CI} for this difference is a mere $(\Sexpr{I(ci_hr)})$ at a \Sexpr{alpha*100}\% confidence level, fairly small for the purposes of network traffic. Additionally, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms with a latency of 20 ms).

\FloatBarrier
\subsection{Invalid Loss Rate}
\par Tests 5 through 8 include invalid traffic that \ac{ARG} should reject and hence should be ``lost.'' Figure \ref{fig:basic_invalid_loss} displays the percentage of invalid traffic rejected. \Sexpr{alpha*100}\% \ac{CI} are theoretically displayed, although all but one test showed 100\% invalid packet loss.

\begin{figure}
\caption{Invalid Traffic Loss Rate \tbd{add \% to numbers}}
\label{fig:basic_invalid_loss}
\centering
<<basic_invalid_loss, out.width=".75\\textwidth", echo=FALSE, warning=FALSE>>=
# Show loss rate of invalid traffic for tests 5-8.
invalid_runs = !valid_runs
plotmeans(basic[invalid_runs, 'invalid.loss.rate']~basic[invalid_runs, 'test'], p=alpha,
		minbar=0, maxbar=100, ylim=c(99.5, 100),
		xlab="Test Number", ylab="Mean Invalid Packet Loss Rate (Percentage)")
@
\end{figure}

\par Figure \ref{fig:basic_invalid_loss} indicates that \ac{ARG} successfully rejects all invalid traffic it encounters. Test 8 shows a small deviation from the expected 100\% packet rejection, as 1 of 746 packets made it through, but further examination of this number shows a rare post-processing problem with identical packets sent within seconds of each other (the problem lies with the log analyzer, not \ac{ARG} itself). This result offers convincing evidence that \ac{ARG} effectively blocks unexpected inbound traffic, but it gives no indication of its suitablity against more focused attacks. 

\tbd{Discuss misleading numbers in this table. Make percentages?}
<<echo=FALSE, results='asis'>>=
# Get the total for each possible rejection reason for the invalid tests only
create_rejection_table('basic_loss_reasons_invalid', 'Basic Tests 5-8 Packet Rejection Reasons', basic[invalid_runs,])
@

\par Additionally, Table \ref{tbl:basic_loss_reasons_invalid} illustrates that packets are rejected via the \ac{NAT} table quite frequently. This operation costs the gateway minimal processing time, as it can reject after a single hashtable lookup. The less time wasted on invalid packets the better, as it lowers the possibility of a \ac{DOS} attack; if all bandwith is consumed there is still a problem, but that is beyond the control of \ac{ARG}.

\section{Maximum Hop Rate}
\label{sec:results_hoprate_tests}
\par The maximum hop rate sequence of tests measures the change in packet loss rate at specific latencies as hop rate increases. For these tests, a fixed packet rate and Test 4 is always used. Section \ref{sec:exp_design} covers the specifics of these tests.

\begin{figure}
\caption{Loss Rate of Latency vs. Hop Rate}
\label{fig:hoprate_loss}
\centering
<<echo=FALSE>>=
# Get mean of each hop rate-latency pair. Matrix
# rows are latencies, columns are hop rates
lat_hr_mean = matrix(nrow=length(unique(hoprate[,'latency'])),
	ncol=length(unique(hoprate[,'hop.rate'])))
rownames(lat_hr_mean) = sort(unique(hoprate[,'latency']))
colnames(lat_hr_mean) = sort(unique(hoprate[,'hop.rate']))

worst_sd = 0
repetitions = c()
for(lat in rownames(lat_hr_mean)) {
	for(hr in colnames(lat_hr_mean)) {
		rows = hoprate[,'latency'] == lat & hoprate[,'hop.rate'] == hr

		# Calculate mean for each test number
		#b = boot(data=hoprate[rows,], statistic=function(d, i) {
		#			return(c(mean(d[i], na.rm=TRUE), sd(d[i], na.rm=TRUE)))
		#		}, R=replicates)

		lat_hr_mean[lat, hr] = mean(hoprate[rows, 'valid.loss.rate'])
		#lat_hr_mean[lat, hr] = b$t0

		repetitions = append(repetitions, length(hoprate[rows, 'valid.loss.rate']))

		curr_sd = sd(hoprate[rows, 'valid.loss.rate'])
		#curr_sd = b$t1
		if(!is.na(curr_sd) && curr_sd > worst_sd)
			worst_sd = curr_sd
	}
}

# Plot. Each latency is separate, x is hop rate, y is loss
old_par = par(no.readonly=TRUE)
par(mar=c(5.1, 4.1, 4.1, 7.8))

plot.new()
xlim = c(1, ncol(lat_hr_mean))
ylim = c(0, max(lat_hr_mean, na.rm=TRUE))
plot.window(xlim=xlim, ylim=ylim)
title(main="Hop Interval vs. Loss Rate", xlab="Hop Interval (ms)", ylab="Valid Packet Loss Rate")
axis(1, at=1:ncol(lat_hr_mean), labels=colnames(lat_hr_mean))
yaxis = c(0, reasonable, seq(from=5, to=max(ylim), by=5))
axis(2, at=yaxis, labels=paste(yaxis, '%', sep=''))
box()

abline(h=reasonable, lty=5, col="purple")

type=1
for(lat in rownames(lat_hr_mean)) {
	lines(lat_hr_mean[lat,], col=type, lty=type)
	points(lat_hr_mean[lat,], col=type, pch=type-1)
	type = type + 1
}

legend(title="Latency (RTT)",
		xpd=TRUE,
		#x="bottomleft",
		x=xlim[2] + .6,
		y=mean(ylim) + 3,
		legend=c(paste(rownames(lat_hr_mean), "ms"), 'Reasonable\nLoss %'),
		lty=c(1:(type-1), 5),
		col=c(1:(type-1), "purple"),
		pch=c(0:(type-2), NA)
		)

par(old_par)
@
\end{figure}

\tbd{this number includes both to ext and between gateways. We really care about just gateways for this purpose. We should have no loss to ext. Need to have process run produces separate loss rates for between gateways, should only require changes to getting stats. Then do two graphs, one showing no change on prot<->ext and the changes in prot<->prot. Also create separate graph for TCP and UDP}

\par Figure \ref{fig:hoprate_loss} displays the results of the hop rate tests, separated by network latency. The values shown here are the bootstrapped means over \Sexpr{round(mean(repetitions))} test repetitions with \Sexpr{replicates} replicates used in the bootstrap. The worst standard deviation in any of the hop rate-latency groups is \Sexpr{worst_sd}, implying some variation in each hop rate-latency grouping but still relatively predictable losses.

\par Interestingly, the zero millisecond latency trials exhibit a noticably different decline than the others, with zero milliseconds giving a relatively steady decline in loss rate and the others remaining relatively flat before a rapid drop. This curve change is likely due to the test environment causing a ``triple latency'' problem.

\par As discussed in Section \ref{sec:eth_routing}, Ethernet uses \ac{ARP} requests and responses to transfer packets between machines on the same network segment. In the test environment, the gateways and the external host are all on the same segment and hence must work with all the extra Ethernet frames that entails.  On a normal Ethernet segment with minimal latency (the zero millisecond test), the \ac{ARP} process takes minimal time and has little impact on operation. To speed the process even further, \ac{ARP} data is cached, so when a system wants to send to the same \ac{IP} again, it references the ARP cache table and does not have to go through the request-response process again. With fast hop rates, however, gateways frequently need to send to different \ac{IP} addresses and therefore must send and receive the requiste \acp{ARP} very frequently (generally, once every hop). When the network latency is not zero, these additional frames can cause siginficant delays. Figure \ref{fig:triple_arp_issue} illustrates what happens when a time synchronization between gateways occurs with a five millisecond one-way latency.

\begin{figure}
\caption{Time Synchronization, Including \ac{ARP}s \tbd{why you no 2 columns?}}
\label{fig:triple_arp_issue}
\centering
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue}
\caption{With 5 millsecond \ac{ARP}}
\end{subfigure}
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue_no_arp}
\caption{With 0 millsecond \ac{ARP} (not shown)}
\end{subfigure}
\end{figure}

\par If there were no need for these ARP requests (or they were negligible), then the entire time sync process would complete in 10 milliseconds. Because of the \acp{ARP}, however, it takes 30 milliseconds. The time base for the other gateway is still (usually) calculated correctly, because the latency of that particular packet is taken into account. Later packets may take a wide variety of latencies, however, because they may not require any \acp{ARP}, only one direction might require it, or both directions may need it. For a 5 millisecond one-way latency, this only leads to variantions of 5, 10, or 15 milliseconds, a completely reasonabl amount of variation. By the time tests get to 100 milliseconds latencies, one-way latencies easily range from 100 to 300 milliseconds, with \ac{RTT} of around 600 milliseconds. At faster hop rates, the triple latency problem is encountered more frequently. This degree of variation is difficult to compensate for, leading to siginificant packet loss.


\par Why then do losses suddenly drop when hop rate is twice the latency? If the triple latency problem is ignored, it would be reasonable to expect that when the hop rate is at least the one-way latency communication can occur (assuming stable latencies), as hypothesized in Chapter \ref{chp:methodology}. This time frame should work out because it allows a packet to cross the network from one gateway to another before the receiver changes \acp{IP} twice, even if the packet is sent just before the receiver hops the first time (as mentioned in Chapter \ref{chp:implementation}, gateways accept packets at the current and previous \ac{IP}). Any faster hop rate results in a period of time before each hop that guarrantees sent packets arrive too late. 

\tbd{Figure out why. Edit from here}

\par The no-latency test still maintains around 5\% loss at very rapid hop rates, but does not cross below \Sexpr{reasonable}\% until hops take at least 30 milliseconds. This indicates a maximum supportable hop rate of around 30-40 ms, even when latency is taken out of the picture.

\par As expected, far and away the most common rejection reason for this set of tests is incorrect source and/or destination \acp{IP}, as shown in Table \ref{tbl:hoprate_loss_reasons}. Other entries in this table are in Section \ref{sec:basic}.

\par With a realistic \ac{RTT} network latency of 30 milliseconds, these tests prove it is possible to change \acp{IP} at least every 50 milliseconds. Previous research into address space randomization and \ac{IP} hopping have limited themselves to hopping on the order of \tbd{look this up}.

<<echo=FALSE, results='asis'>>=
create_rejection_table('hoprate_loss_reasons', 'Hoprate Loss Reasons', hoprate)
@

\section{Maximum Packet Rate}
\label{sec:results_packetrate_tests}
\par The final numerical test performed against \ac{ARG} tests the maximum packet rate it is capable of handling. For these tests, latency is set to 20 milliseconds and Test 4 is used. Hop rate varies between 50 ms and 500 ms to check if this has an effect on the supported packet rate. Most importantly, traffic generators are run with steadily increasing send rates. See Section \ref{sec:exp_design} for details.

\begin{figure}
\begin{subfigure}[b]{0.5\linewidth}
\centering
<<echo=FALSE>>=
# Plot 
max_y = .4

packetrate = packetrate[order(packetrate[,'kbps']),]
packetrate[,'mbps'] = packetrate[,'kbps']/1024
plot(packetrate[,'mbps'], packetrate[,'valid.loss.rate'], ylim=c(0,max_y),
	xlab="Throughput (Mbps)", ylab="Valid Packet Loss Rate, Percentage")
lines(packetrate[,'mbps'], packetrate[,'valid.loss.rate'])
@
\caption{Loss Rate vs. \ac{Mbps} Throughput}
\label{fig:pr_bps_loss}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}[b]{0.5\linewidth}
\centering
<<echo=FALSE>>=
packetrate = packetrate[order(packetrate[,'pps']),]
plot(packetrate[,'pps'], packetrate[,'valid.loss.rate'], ylim=c(0,max_y),
	xlab="Packet Rate (pps)", ylab="Valid Packet Loss Rate, Percentage")
lines(packetrate[,'pps'], packetrate[,'valid.loss.rate'])
@
\caption{Loss Rate vs. Packets Per Second}
\label{fig:pr_pps_loss}
\end{subfigure}
\end{figure}

\par Figure \ref{fig:pr_bps_loss} depicts the results from this test when looking at bit-wise throughput, while Figure \ref{fig:pr_pps_loss} compares the packet rate. \tbd{cluster, run anova to determine if there's a difference in groups?} Visually, both plots reveals little variation directly attributable to packet rate. \tbd{Clustering the data and performing an \ac{ANOVA} reveales...}

\par The results of these tests are limited to just the implementation of \ac{ARG} and imply only a minimally possible throughput. Despite these limitations, \ac{ARG} performs well with reasonably rapid traffic, giving no reason to believe it is not capable of higher rates.

\section{Fuzzing Test}
\label{sec:results_fuzzer_tests}
\par The final series of tests run on \ac{ARG} consist of throwing invalid and replayed traffic against the gateways. The primary objective is to verify that \ac{ARG} remains stable and able to pass valid traffic while under attack. 

\par The results from these runs are shown in Figure \ref{fig:fuzzer_loss_rate}. 

<<echo=FALSE>>=
# Calculate overall average
fuzzer.boot.overall = boot(data=fuzzer[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
fuzzer.boot.overall.stat = fuzzer.boot.overall$t0
fuzzer.boot.overall.ci = boot.ci(fuzzer.boot.overall, conf=alpha, type='fuzzer')$fuzzer[4:5]

# Calculate mean for each test number
fuzzer.boot = by(fuzzer[,'valid.loss.rate'], fuzzer[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
fuzzer.boot.stats = sapply(fuzzer.boot, function(x) x$t0)
fuzzer.boot.ci = sapply(fuzzer.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='fuzzer')$fuzzer
	return(ci[4:5])
	})
@

\begin{figure}
\caption{Fuzz Testing Loss Rates \tbd{tbd}}
\label{fig:fuzzer_loss_rate}
\centering
<<echo=FALSE>>=
ylim=c(0, max(fuzzer[,'valid.loss.rate'], na.rm=TRUE))

plot(fuzzer[,'valid.loss.rate'],
	xlab="Repetition",
	ylab="Valid Packet Loss Rate (Percentage)", ylim=ylim, yaxt='n')
ylab = seq(0, max(ylim), 5)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}

\par Unfortunately, the results processor this thesis uses to generate statistics on runs is unable to process the logs from the malicious traffic generators and the malformed data in the pcap files are unusable, from its perspective. This makes it difficult to validate the accuracy of results from the runs. Nonetheless, the \tbd{\%} lost over each run indicates a significant difficulty encountered when faced with malicious traffic and a manual examination of the log files does show a large portion of packets being rejected. Table \ref{tbl:fuzzer_loss_reasons} gives the reasons behind each rejection.

<<results='asis'>>=
#create_rejection_table('fuzzer_loss_reasons', 'Fuzzer Packet Reject Reasons', fuzzer)
@

\section{Overall Analysis}
\label{sec:results_overall}
\tbd{answer the questions posed in chapter 4}

\section{Summary}
\par This chapter analysizes the results for each test sequence. Interetation of the results is given, as well as limitations on scope and applicability to the broader field of address space randomization. 

