\chapter{Results and Analysis}
\label{chp:results}
\par This chapter presents and analyzes the experimental results. Each test sequence is discussed in a separate section, with Section \ref{sec:results_basic_tests} covering the basic functionality tests, Section \ref{sec:results_hoprate_tests} determining the effects of hop rate and latency, and Section \ref{sec:results_packetrate_tests} discussing \ac{ARG}'s performance. Section \ref{sec:results_fuzzer_tests} covers the results of running a fuzzer against \ac{ARG}. Finally, Section \ref{sec:results_overall} revists the research questions posed in Chapter \ref{chp:methodology} and summarizes the results with respect to each of them.

<<echo=FALSE, message=FALSE>>=
library(gplots)
library(plotrix)
library(boot)
library(coin)
library(cluster)
options(scipen=10)
@

<<echo=FALSE>>=
alpha=.95
reasonable=2
replicates=1000

# Bring in CSV file and break out into each test, excluding the results.dir and
# label columns
results = read.csv('results.csv', header=TRUE)
results = results[,1:ncol(results)-1] # Remove blank column at end

# Proportions are small and silly
results[1:5,'valid.loss.rate']
loss_cols = c(colnames(results)[startsWith(colnames(results), 'valid.loss.rate')], 'invalid.loss.rate')
loss_cols
results[,loss_cols] = results[,loss_cols] * 100
results[1:5,'valid.loss.rate']

# Tests were 0-indexed. Make them 1-indexed for readability
#results[,'test'] = results[,'test'] + 1

# Convert ms values to integeres
results[,'hop.rate'] = as.integer(sapply(results[,'gatea.hop.rate'], function(x) {
		x = as.character(x);
		substr(x, 1, nchar(x)-2);
	}))

# Combine stats to make them easy to access
results[,'kbps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.kbps'], x['gateb.kbps'], x['gatec.kbps'])
	)) })
results[,'pps'] = apply(results, 1, function(x) { mean(as.integer(
		c(x['gatea.pps'], x['gateb.pps'], x['gatec.pps'])
	)) })

results = split(results, results['label'])
basic = results$basic
hoprate = results$hoprate
packetrate = results$packetrate
fuzzer = results$fuzzer
@

\section{Basic Tests}
\label{sec:results_basic_tests}
\par The first set of tests run tests the basic functioning of \ac{ARG}, as discussed in Section \ref{sec:exp_design}. These tests are intended to validate the basic functionality of \ac{ARG} and demonstrate the accuracy of classification under various circumstances, so relatively packet rates are kept low. Hop rates vary between 50 milliseconds and 500 milliseconds, with round-trip latency fixed at 20 milliseconds.

<<echo=FALSE>>=
bootstrap_stats = function (data, idx) {
	m = mean(data[idx], na.rm=TRUE)
	return(m)
}

# Calculate overall average
basic.boot.overall = boot(data=basic[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
basic.boot.overall.stat = basic.boot.overall$t0
basic.boot.overall.ci = boot.ci(basic.boot.overall, conf=alpha, type='basic')$basic[4:5]

# Calculate mean for each test number
basic.boot = by(basic[,'valid.loss.rate'], basic[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
basic.boot.stats = sapply(basic.boot, function(x) x$t0)
basic.boot.ci = sapply(basic.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='basic')$basic
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
boot.t.test = function(x, y, R, n=10, conf=.95) {
	# Create replicates
	x_resampled = list()
	y_resampled = list()
	for(i in 1:R) {
		x_resampled[[i]] = sample(x, n)
		y_resampled[[i]] = sample(y, n)
	}

	# Test
	t = c()
	for(i in 1:R) {
		t = c(t, t.test(x_resampled[[i]], y_resampled[[i]], conf.level=conf)$statistic)
	}

	# Confidence intervals
	t = sort(t)
	cutoff = floor((1-conf) * length(t))
	return(c(t[cutoff], t[length(t) - cutoff]))
}

basic.hr = split(basic, basic[,'hop.rate'])
basic.hr.t = boot.t.test(basic.hr$'50'[,'valid.loss.rate'], basic.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
basic.hr.pvalue = pvalue(oneway_test(basic[,'valid.loss.rate']~factor(basic[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]

# Utility to show why packets were rejected
create_rejection_table = function(name, caption, table) {
	cat('\\begin{table}[H]\n')
	cat('\\caption{')
	cat(caption)
	cat('}\n')
	cat('\\label{tbl:')
	cat(name)
	cat('}\n')
	cat('\\centering\n')
	cat('\\begin{tabular}{l|c|c}\n')
	cat('Reason & Count & \\% of Total Packets\\\\\n')
	cat('\\hline\n')

	# Total number of packets
	total_packets = sum(table[,'valid.sent'], table[,'invalid.sent'])

	# Sum just the rejection reason columns
	reasons = table[, sapply(colnames(table), function(x) {
			return(startsWith(x, 'inbound') || startsWith(x, 'outbound')
					|| x == 'unknown' || x == 'lost.on.wire')
		})]
	reasons = apply(reasons, 2, function(x) sum(x, na.rm=TRUE))
	reasons = sort(reasons, decreasing=TRUE)

	for(i in 1:length(reasons)) {
		if(is.na(reasons[i]) || reasons[i] == 0)
			next
		
		r = names(reasons)[i]
		r = gsub('.', ' ', r, fixed=TRUE)
		r = paste(toupper(substr(r, 1, 1)), substring(r, 2), sep='')

		cat(r, ' & ', reasons[i], ' & ', reasons[i]/total_packets*100, '\\%', sep='')
		if(i < length(reasons))
			cat('\\\\\n')
	}

	cat('\\end{tabular}\n')
	cat('\\end{table}\n')
}
@

\subsection{Valid Packet Loss}
\par The raw results for the loss of valid packets on each test are shown in Figure \ref{fig:basic_raw_data}. Figure \ref{fig:basic_mean_ci} shows the mean and \acp{CI} of each test, using a \Sexpr{alpha*100}\% confidence level. Due to a lack of normality in the experimental results, these numbers are calculated via bootstrapping with \Sexpr{replicates} replicates.

\begin{figure}
\caption{Basic Test Data}
\begin{subfigure}[t]{0.5\linewidth}
\centering
<<echo=FALSE>>=
# Graph and generate stats on basic tests with test num vs loss rate 
ylim=c(0, max(basic[,'valid.loss.rate'], na.rm=TRUE))

plot(basic[,'test'], basic[,'valid.loss.rate'],
	xlab="Test Number", xaxt='n',
	ylab="Valid Packet Loss", ylim=ylim, yaxt='n')
axis(1, at=names(basic.boot), labels=names(basic.boot))
ylab = seq(0, max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\caption{Raw Data}
\label{fig:basic_raw_data}
\end{subfigure}
\hspace{0.5cm}
\begin{subfigure}[t]{0.5\linewidth}
\centering
<< echo=FALSE, warning=FALSE>>=
plot.new()
plot.window(xlim=c(0, length(basic.boot)-1), ylim=ylim)
title(main="", xlab="Test Number", ylab="Mean Valid Packet Loss")
axis(1, at=names(basic.boot), labels=names(basic.boot))
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
box()

points(0:(length(basic.boot)-1), basic.boot.stats)
plotCI(0:(length(basic.boot)-1), basic.boot.stats,
	ui=sapply(basic.boot.ci[2,], function(x) min(x, 1)),
	li=sapply(basic.boot.ci[1,], function(x) max(x, 0)),
	add=TRUE, scol='blue')

#text(0:(length(basic.boot)-1) + .5, basic.boot.stats,
#	labels=sapply(basic.boot.stats, function(x) round(x, digits=4)))
@
\caption{Mean and \ac{CI}}
\label{fig:basic_mean_ci}
\end{subfigure}
\end{figure}

\par These figures reveal a low packet loss across all test scenarios. At worst, Test 8---traffic in all directions, including invalid traffic---lost between \Sexpr{basic.boot.ci[1,9]}\% and \Sexpr{basic.boot.ci[2,9]}\% of packets, with \Sexpr{alpha*100}\% confidence. Across all tests, valid packet loss averaged \Sexpr{basic.boot.overall.stat}\%, with a \ac{CI} of $(\Sexpr{I(basic.boot.overall.ci)})$, again with \Sexpr{alpha*100}\% confidence. These results confirm Chapter \ref{chp:methodology}'s hypothesis that \ac{ARG} causes packet loss less than \Sexpr{reasonable}\%, at least under normal conditions.

\par Figure \ref{fig:basic_raw_data} displays two outliers on Test 3 and an occasional outlier in Test 5, 6, and 7. Examining the data reveals the dropped packets exceeded the maximum size of a packet \ac{ARG} can pass between gateways. As Chapter \ref{chp:implementation} covers, \ac{ARG} wraps packets between gateways in its own headers, increasing the size of the packet. Ethernet II has a default maximum transmission unit of 1500, so \ac{ARG} drops packets that pass this limit after being wrapped. 

\par A resampling of the data between the two hop rates used in the basic tests gives a p-value of \Sexpr{basic.hr.pvalue}, convincing evidence that there is a statistically significant difference introduced by the different hop rates. However, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms, just barely twice the 20 ms latency). Ideally these tests would be re-run with a hop rate closer to 100 milliseconds to avoid interaction with the latency.

\begin{comment}
<<echo=FALSE, results='asis'>>=
valid_runs = apply(basic, 1, function(x) x['test'] < 5)
create_rejection_table('basic_loss_reasons_all', 'Basic Tests 1-4 Packet Reject Reasons', basic[valid_runs,])
@
\end{comment}

%However, the \ac{CI} for this difference is a mere $(\Sexpr{I(ci_hr)})$ at a \Sexpr{alpha*100}\% confidence level, fairly small for the purposes of network traffic. Additionally, the results in Section \ref{sec:results_hoprate_tests} indicate this difference is due to the hop rate-latency interaction, which just barely exceeds the requirement for reliable communication (in this test, the hop interval is 50 ms with a latency of 20 ms).

\FloatBarrier
\subsection{Invalid Loss Rate}
\par Tests 5 through 8 include invalid traffic that \ac{ARG} should reject and hence should be ``lost.'' Figure \ref{fig:basic_invalid_loss} displays the percentage of invalid traffic rejected, clearly revealing . As shown, over the course of nearly 30 repetitions, no test allowed any invalid packets through. %\Sexpr{alpha*100}\% \ac{CI} are theoretically displayed, but all showed 100\% invalid packet loss.

\begin{figure}
\caption{Invalid Traffic Loss Rate}
\label{fig:basic_invalid_loss}
\centering
<<basic_invalid_loss, out.width=".75\\textwidth", echo=FALSE, warning=FALSE>>=
# Show loss rate of invalid traffic for tests 5-8.
invalid_runs = !valid_runs
ylim = c(99.9, 100)
plotmeans(basic[invalid_runs, 'invalid.loss.rate']~basic[invalid_runs, 'test'], p=alpha,
		connect=FALSE,
		minbar=0, maxbar=100, ylim=ylim, yaxt='n',
		xlab="Test Number", ylab="Invalid Packet Loss")
ylab = seq(0, max(ylim), .01)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}

%\par Figure \ref{fig:basic_invalid_loss} indicates that \ac{ARG} successfully rejects all invalid traffic it encounters. Test 8 shows a small deviation from the expected 100\% packet rejection, as 1 of 746 packets made it through, but further examination of this number shows a rare post-processing problem with identical packets sent within seconds of each other (the problem lies with the log analyzer, not \ac{ARG} itself). This result offers convincing evidence that \ac{ARG} effectively blocks unexpected inbound traffic, but it gives no indication of its suitablity against more focused attacks. 

\tbd{Discuss misleading numbers in this table. Make percentages?}
<<echo=FALSE, results='asis'>>=
# Get the total for each possible rejection reason for the invalid tests only
create_rejection_table('basic_loss_reasons_invalid', 'Basic Tests 5-8 Packet Rejection Reasons', basic[invalid_runs,])
@

\par Additionally, Table \ref{tbl:basic_loss_reasons_invalid} illustrates that packets are rejected via the \ac{NAT} table quite frequently. This operation costs the gateway minimal processing time, as it can reject after a single hashtable lookup. The less time wasted on invalid packets the better, as it lowers the possibility of a \ac{DOS} attack; if all bandwith is consumed there is still a problem, but that is beyond the control of \ac{ARG}.

\par \tbd{waiting for new results}

\FloatBarrier
\section{Maximum Hop Rate}
\label{sec:results_hoprate_tests}
\par The maximum hop rate sequence of tests measures the change in packet loss rate at specific latencies as hop rate increases. For these tests, a fixed packet rate and Test 4 is always used. Section \ref{sec:exp_design} covers the specifics of these tests. The figures below document the results of the tests, broken up by traffic type and direction.

<<echo=FALSE>>=
plot_latency_hoprate = function(hoprate, loss_rate_col, title='', xlim=NULL, ylim=NULL, yby=2) {
	# Get mean of each hop rate-latency pair. Matrix
	# rows are latencies, columns are hop rates
	lat_hr_mean = matrix(nrow=length(unique(hoprate[,'latency'])),
						ncol=length(unique(hoprate[,'hop.rate'])))
	rownames(lat_hr_mean) = sort(unique(hoprate[,'latency']))
	colnames(lat_hr_mean) = sort(unique(hoprate[,'hop.rate']))

	worst_sd = 0
	repetitions = c()
	for(lat in rownames(lat_hr_mean)) {
		for(hr in colnames(lat_hr_mean)) {
			rows = hoprate[,'latency'] == lat & hoprate[,'hop.rate'] == hr

			# Calculate mean for each test number
			#b = boot(data=hoprate[rows,], statistic=function(d, i) {
			#			return(c(mean(d[i], na.rm=TRUE), sd(d[i], na.rm=TRUE)))
			#		}, R=replicates)

			lat_hr_mean[lat, hr] = mean(hoprate[rows, loss_rate_col])
			#lat_hr_mean[lat, hr] = b$t0

			repetitions = append(repetitions, length(hoprate[rows, loss_rate_col]))

			curr_sd = sd(hoprate[rows, loss_rate_col])
			#curr_sd = b$t1
			if(!is.na(curr_sd) && curr_sd > worst_sd)
				worst_sd = curr_sd
		}
	}

	# Plot. Each latency is separate, x is hop rate, y is loss
	old_par = par(no.readonly=TRUE)
	par(mar=c(5.1, 4.1, 4.1, 7.8))

	plot.new()
	if(is.null(xlim))
		xlim = c(1, ncol(lat_hr_mean))
	if(is.null(ylim))
		ylim = c(0, max(lat_hr_mean, na.rm=TRUE))
	plot.window(xlim=xlim, ylim=ylim)
	title(main=title, xlab="Hop Interval (ms)", ylab="Packet Loss")

	axis(1, at=1:ncol(lat_hr_mean), labels=colnames(lat_hr_mean))
	yaxis = c(reasonable, seq(from=min(ylim), to=max(ylim), by=yby))
	axis(2, at=yaxis, labels=paste(yaxis, '%', sep=''))

	box()

	abline(h=reasonable, lty=5, col="purple")

	type=1
	for(lat in rownames(lat_hr_mean)) {
		lines(lat_hr_mean[lat,], col=type, lty=type)
		points(lat_hr_mean[lat,], col=type, pch=type-1)
		type = type + 1
	}

	legend(title="Latency (RTT)",
			xpd=TRUE,
			x=xlim[2] + .6,
			y=mean(ylim) + (ylim[2] - ylim[1])/5,
			legend=c(paste(rownames(lat_hr_mean), "ms"), paste('Reasonable\nLoss: ', reasonable, '%', sep='')),
			lty=c(1:(type-1), 5),
			col=c(1:(type-1), "purple"),
			pch=c(0:(type-2), NA)
			)

	par(old_par)

	return(c(sd=worst_sd, repetitions=mean(repetitions)))
}
@

\begin{figure}
\caption{Loss Rate, \ac{ARG} Network Traffic}
\label{fig:hrlat_udptcp_inter_full}
\centering
<<echo=FALSE>>=
# tbd change to valid.loss.rate.udptcp.interarg
plot_results = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', yby=5)
worst_sd = plot_results['sd']
repetitions = plot_results['repetitions']
@
\end{figure}

\begin{figure}
\caption{Loss Rate, \ac{ARG} Network Traffic, Scaled}
\label{fig:hrlat_udptcp_inter_zoomed}
\centering
<<echo=FALSE>>=
# tbd change to valid.loss.rate.udptcp.interarg
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.udptcp.interarg', ylim=c(0, 5), yby=.5)
@
\end{figure}

\par Figure \ref{fig:hrlat_udptcp_inter_full} displays the results of the hop rate tests, separated by network latency. Figure \ref{fig:hrlat_udptcp_inter_zoomed} displays the same numbers, but with a much tighter Y scale to reveal differences at large hop intervals. The numbers shown here only include packets sent between \ac{ARG}-protected clients, traffic to the external host and administrative packets between gateways are not included. The values given are the bootstrapped means over \Sexpr{round(repetitions)} test repetitions with \Sexpr{replicates} replicates used in the bootstrap. \tbd{not true right now}. The worst standard deviation in any of the hop rate-latency groups is \Sexpr{worst_sd}, implying some variation in each hop rate-latency grouping but still relatively predictable losses.

\begin{figure}
\caption{Loss Rate, TCP Packets Only}
\label{fig:hrlat_interarg}
\centering
<<echo=FALSE>>=
# tbd change to valid.loss.rate.tcp.interarg
a = plot_latency_hoprate(hoprate, 'valid.loss.rate.tcp.interarg', yby=.5)
@
\end{figure}

\par Interestingly, the zero millisecond latency trials exhibit a noticably different decline than the others, with zero milliseconds giving a relatively steady decline in loss rate and the others remaining relatively flat before a rapid drop. This curve change is likely due to the test environment causing a ``triple latency'' problem.

\par As discussed in Section \ref{sec:eth_routing}, Ethernet uses \ac{ARP} requests and responses to transfer packets between machines on the same network segment. In the test environment, the gateways and the external host are all on the same segment and hence must work with all the extra Ethernet frames that entails.  On a normal Ethernet segment with minimal latency (the zero millisecond test), the \ac{ARP} process takes minimal time and has little impact on operation. To speed the process even further, \ac{ARP} data is cached, so when a system wants to send to the same \ac{IP} again, it references the ARP cache table and does not have to go through the request-response process again. With fast hop rates, however, gateways frequently need to send to different \ac{IP} addresses and therefore must send and receive the requiste \acp{ARP} very frequently (generally, once every hop). When the network latency is not zero, these additional frames can cause siginficant delays. Figure \ref{fig:triple_arp_issue} illustrates what happens when a time synchronization between gateways occurs with a five millisecond one-way latency.

\begin{figure}
\caption{Time Synchronization, Including \ac{ARP}s}
\label{fig:triple_arp_issue}
\centering
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue}
\caption{With 5 millsecond \ac{ARP}}
\end{subfigure}
\begin{subfigure}[t]{.6\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{triple_arp_issue_no_arp}
\caption{With 0 millsecond \ac{ARP} (not shown)}
\end{subfigure}
\end{figure}

\par If there were no need for these ARP requests (or they were negligible), then the entire time sync process would complete in 10 milliseconds. Because of the \acp{ARP}, however, it takes 30 milliseconds. The time base for the other gateway is still (usually) calculated correctly, because the latency of that particular packet is taken into account. Later packets may take a wide variety of latencies, however, because they may not require any \acp{ARP}, only one direction might require it, or both directions may need it. For a 5 millisecond one-way latency, this only leads to variantions of 5, 10, or 15 milliseconds, a completely reasonabl amount of variation. By the time tests get to 100 milliseconds latencies, one-way latencies easily range from 100 to 300 milliseconds, with \ac{RTT} of around 600 milliseconds. At faster hop rates, the triple latency problem is encountered more frequently. This degree of variation is difficult to compensate for, leading to siginificant packet loss.


\par Why then do losses suddenly drop when hop rate is twice the latency? If the triple latency problem is ignored, it would be reasonable to expect that when the hop rate is at least the one-way latency communication can occur (assuming stable latencies), as hypothesized in Chapter \ref{chp:methodology}. This time frame should work out because it allows a packet to cross the network from one gateway to another before the receiver changes \acp{IP} twice, even if the packet is sent just before the receiver hops the first time (as mentioned in Chapter \ref{chp:implementation}, gateways accept packets at the current and previous \ac{IP}). Any faster hop rate results in a period of time before each hop that guarrantees sent packets arrive too late. 

\tbd{Figure out why. Edit from here}

\par The no-latency test still maintains around 5\% loss at very rapid hop rates, but does not cross below \Sexpr{reasonable}\% until hops take at least 30 milliseconds. This indicates a maximum supportable hop rate of around 30-40 ms, even when latency is taken out of the picture.

\par As expected, far and away the most common rejection reason for this set of tests is incorrect source and/or destination \acp{IP}, as shown in Table \ref{tbl:hoprate_loss_reasons}. Other entries in this table are in Section \ref{sec:basic}.

\par With a realistic \ac{RTT} network latency of 30 milliseconds, these tests prove it is possible to change \acp{IP} at least every 50 milliseconds. Previous research into address space randomization and \ac{IP} hopping have limited themselves to hopping on the order of \tbd{look this up}.

<<echo=FALSE, results='asis'>>=
create_rejection_table('hoprate_loss_reasons', 'Hoprate Loss Reasons', hoprate)
@

\section{Maximum Packet Rate}
\label{sec:results_packetrate_tests}
\par The final numerical test performed against \ac{ARG} tests the maximum packet rate it is capable of handling. For these tests, latency is set to 20 milliseconds and Test 4 is used. Hop rate varies between 50 ms and 500 ms to check if this has an effect on the supported packet rate. Most importantly, traffic generators are run with steadily increasing send rates. See Section \ref{sec:exp_design} for details. Because throughput varies somewhat on each run, the data is clustered into the groupings shown in Table \ref{tbl:pr_clusters}.

<<echo=FALSE>>=
# Sort base on throughput
packetrate = packetrate[order(packetrate[,'kbps']),]

# Cluster data by kbps
cols = c('kbps', 'valid.loss.rate')
packetrate.fit = kmeans(packetrate[,cols], 6, nstart=100)
packetrate[,'bps.clust'] = packetrate.fit$cluster

# Calculate mean kbps for each throughput cluster
packetrate.boot = by(packetrate[,'valid.loss.rate'], packetrate[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
packetrate.boot.stats = sapply(packetrate.boot, function(x) x$t0)
packetrate.boot.ci = sapply(packetrate.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='packetrate')$packetrate
	return(ci[4:5])
	})

# Calculate 2-sample t-test for <R> bootstrap samples of <x> and <y>
packetrate.hr = split(packetrate, packetrate[,'hop.rate'])
packetrate.hr.t = boot.t.test(packetrate.hr$'50'[,'valid.loss.rate'], packetrate.hr$'500'[,'valid.loss.rate'], R=replicates, conf=alpha)

# Are the two hop rates different?
packetrate.hr.pvalue = pvalue(oneway_test(packetrate[,'valid.loss.rate']~factor(packetrate[,'hop.rate']),
							distribution='approximate', conf.level=alpha))[1]
@

\begin{table}
\caption{Packet Rate Clusters}
\label{tbl:pr_clusters}
\centering
\begin{tabular}{c|c|c|c|c}
Num & Mean (Kbps) & Min (Kbps) & Max (Kbps) & Color\\
\hline
<<echo=FALSE, results='asis'>>=
for(clust in 1:max(packetrate.fit$cluster)) {
	kbps = packetrate[packetrate.fit$cluster == clust,'kbps']
	cat(clust)
	cat(' & ')
	cat(mean(kbps))
	cat(' & ')
	cat(min(kbps))
	cat(' & ')
	cat(max(kbps))
	cat(' & \\\\')
}
@
\end{tabular}
\end{table}

\begin{figure}
\caption{Loss Rate vs. Throughput, Raw Data}
\label{fig:pr_raw_data}
\centering
<<echo=FALSE>>=
# Plot 
ylim = c(0, .4)

clust_colors = 1:max(packetrate.fit$cluster)
plot(packetrate[,'kbps'], packetrate[,'valid.loss.rate'],
	ylim=ylim, yaxt='n',
	col=clust_colors[packetrate.fit$cluster],
	pch=packetrate.fit$cluster - 1,
	xlab="Throughput (Kbps)", ylab="Valid Packet Loss Rate, Percentage",
	sub="Colors indicate clustering")
for(clust in 1:max(packetrate.fit$cluster)) {
	rows = packetrate.fit$cluster == clust
	lines(packetrate[rows,'kbps'], packetrate[rows,'valid.loss.rate'], col=clust_colors[clust])
}

ylab = seq(min(ylim), max(ylim), .05)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))

max_packet_rate = max(packetrate[,'kbps'])
@
\end{figure}

\begin{comment}
\begin{subfigure}[b]{0.5\linewidth}
\centering
<<echo=FALSE>>=
clusplot(packetrate[,cols], packetrate.fit$cluster, color=TRUE, shade=TRUE, lines=0, main='')
@
\caption{Clustered by \ac{Kbps}}
\label{fig:pr_pps_loss}
\end{subfigure}
\end{comment}

\par Figure \ref{fig:pr_raw_data} shows the raw data from these tests, with colors indicating clustering by bit-wase throughput. The data points have been clustered together into \Sexpr{max(packetrate.fit$cluster)} groups using K-means clustering. Table \ref{tbl:pr_clusters} lists the clusters used, the corresponding color, and other related data. Visually, the plot reveals fairly similar loss in each cluster, although there may be a slight trend upward at higher rates. To examine this further a Tukey test is used, the results of which are shown in Figure \ref{fig:pr_tukey}.

\begin{figure}
\caption{Tukey Test Against Clustered Packet Rate Data}
\label{fig:pr_tukey}
<<echo=FALSE>>=
m = aov(valid.loss.rate~factor(bps.clust), packetrate)
t = TukeyHSD(m, conf.level=alpha)
plot(t)
@
\end{figure}

\par The results of these tests are limited to just the implementation of \ac{ARG} and imply only a minimally possible throughput. Despite these limitations, \ac{ARG} performs well with reasonably rapid traffic, giving no reason to believe it is not capable of higher rates.

\section{Fuzzing Test}
\label{sec:results_fuzzer_tests}
\par The final series of tests run on \ac{ARG} consist of throwing invalid and replayed traffic against the gateways. The primary objective is to verify that \ac{ARG} remains stable and able to pass valid traffic while under attack. 

\par The results from these runs are shown in Figure \ref{fig:fuzzer_loss_rate}. 

<<echo=FALSE>>=
# Calculate overall average
fuzzer.boot.overall = boot(data=fuzzer[,'valid.loss.rate'], statistic=bootstrap_stats, R=replicates)
fuzzer.boot.overall.stat = fuzzer.boot.overall$t0
fuzzer.boot.overall.ci = boot.ci(fuzzer.boot.overall, conf=alpha, type='fuzzer')$fuzzer[4:5]

# Calculate mean for each test number
fuzzer.boot = by(fuzzer[,'valid.loss.rate'], fuzzer[,'test'], function(x) {
		boot(data=x, statistic=bootstrap_stats, R=replicates)
	})
fuzzer.boot.stats = sapply(fuzzer.boot, function(x) x$t0)
fuzzer.boot.ci = sapply(fuzzer.boot, function(x) {
	ci = boot.ci(x, conf=alpha, type='fuzzer')$fuzzer
	return(ci[4:5])
	})
@

\begin{figure}
\caption{Fuzz Testing Loss Rates \tbd{tbd}}
\label{fig:fuzzer_loss_rate}
\centering
<<echo=FALSE>>=
ylim=c(0, max(fuzzer[,'valid.loss.rate'], na.rm=TRUE))

plot(fuzzer[,'valid.loss.rate'],
	xlab="Repetition",
	ylab="Valid Packet Loss Rate (Percentage)", ylim=ylim, yaxt='n')
ylab = seq(0, max(ylim), 5)
axis(2, at=ylab, labels=paste(ylab, '%', sep=''))
@
\end{figure}

\par Unfortunately, the results processor this thesis uses to generate statistics on runs is unable to process the logs from the malicious traffic generators and the malformed data in the pcap files are unusable, from its perspective. This makes it difficult to validate the accuracy of results from the runs. Nonetheless, the \tbd{\%} lost over each run indicates a significant difficulty encountered when faced with malicious traffic and a manual examination of the log files does show a large portion of packets being rejected. Table \ref{tbl:fuzzer_loss_reasons} gives the reasons behind each rejection.

<<results='asis'>>=
#create_rejection_table('fuzzer_loss_reasons', 'Fuzzer Packet Reject Reasons', fuzzer)
@

\section{Overall Analysis}
\label{sec:results_overall}
\par Chapter \ref{chp:methodolgy} presents several questions that this research attempts to answer. Each is revisited below along with an accompanying conclusion. More details on each answer may be found in earlier sections of this chapter. 

\begin{enumerate}
\item Does \ac{ARG} classify traffic correctly?
\par Overall, \ac{ARG} does classify traffic correctly. Throughout the testing, false negatives never occured, indicating \ac{ARG}'s \ac{NAT} system is robust and provides a solid way to classify inbound traffic. However, the traffic used to test this system is fairly undirected, with no intentional misleading. Testing needs to be done with a more intelligent threat. 

\par For traffic between \ac{ARG}-protected networks, \ac{ARG} is reasonably accurate. In basic tests involving a range of traffic types, \ac{ARG} tended to lose less than \tbd{when new results are in, ensure this is accurate}

\item What is the max packet rate \ac{ARG} can handle?
\par The testing done for this research did not find an upper limit on the amount of traffic \ac{ARG} could handle, with the gateways showing the capability of handling over \Sexpr{round(max_packet_rate, 1)} \ac{Kbps} with little change in loss rate. While a system deployed in front of a corporate network would need to handle much more throughput, these tests find no reason to believe \ac{ARG} incapable of scaling well.

\item What is the max hop rate---the frequency with which \ac{ARG} changes \acp{IP}---that is supportable? How does latency affect this?
\par \tbd{Oh boy... really need to the new stats here}. This is a sigificant contribution because...

\item Is \ac{ARG} stable when presented with corrupt, malformed, or replayed packets?
\par \ac{ARG} is certainly stable in the face of bad traffic. Long-running fuzz tests do not cause \ac{ARG} to crash, nor do they permanently break traffic flow. However, \tbd{need data}
\end{enumerate}

\section{Summary}
\par This chapter analysizes the results for each test sequence. Interetation of the results is given, as well as limitations on scope and applicability to the broader field of address space randomization. 

